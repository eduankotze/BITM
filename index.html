<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">

  <title>Natural Language Processing with Machine and Deep Learning</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>


<!-- Header -->
<div id="header" style="text-align:center">
  <h1>Natural Language Processing with Machine and Deep Learning</h1>
  <h3>Inspired from <a href="http://web.stanford.edu/class/cs224n/">CS224n by Stanford University</a></h3>
  <br>
  <div style="clear:both;"></div>
</div>


<!-- Intro -->
<div class="container sec" id="intro">
  <p>
    Dear students, I hope the following webpage will assist you in your learning journey about NLP, machine learning and deep learning. <br>
  </p>
</div>


<!-- Content -->
<div class="container sec" id="content">

  <h2>Prerequisites</h2>
  <ul>
      <li><b>Proficiency in Python</b>
          <p>Become familiar with Python 3 (using Numpy, pandas, NLTK, scikit-learn, Keras and PyTorch).</p>
          <p><u>The following resources are recommended if you new to Python Programming:</u></p>
          <ul><p>
              <li>Udemy Python 3 - Learning Python Programming Step-by-Step for Beginners [require login]: <a href="https://www.udemy.com/course/learn-python-programming-a-step-by-step-course-to-beginners/learn/lecture/12935322#content" target="_blank">Udemy course - Python Programming</a></li>
              <li>Udemy Python 3 - Programming basics to advanced: <a href="https://www.udemy.com/course/learn-python-programming-a-step-by-step-course-to-beginners/" target="_blank">Udemy course - Python for Beginners</a></li>
              <li>Youtube (free) - Learn Python in 45 minutes! <a href="https://collab.hpc.ufs.ac.za/index.php/s/cKFTSdtLwnDXyKo" target="_blank">YouTube [74.7MB]</a></li>
          </p></ul>
          
          <p><u>Useful Python Resources:</u></p>
          <ul><p>
              <li>Chris Albon. Python Scripts for Machine Learning and NLP: <a href="https://chrisalbon.com/#machine_learning" target="_blank">https://chrisalbon.com/#machine_learning</a></li>
              <li>Jason Browlee. Machine Learning Mastery with Python: <a href="https://machinelearningmastery.com/" target="_blank">https://machinelearningmastery.com/</a></li>
              <li>MLwhiz. NLP Learning Series: <a href="https://mlwhiz.com/nlpseries/" target="_blank">https://mlwhiz.com/nlpseries/</a></li>
              <li>TowardsDataScience: Machine Learning and NLP: <a href="https://towardsdatascience.com/machine-learning/home" target="_blank">https://towardsdatascience.com/machine-learning/home</a></li>
              <li>scikit-learn: <a href="https://scikit-learn.org/stable/" target="_blank">https://scikit-learn.org/stable/</a></li>
              <li>Keras Github: <a href="https://github.com/keras-team/keras" target="_blank">https://github.com/keras-team/keras</a></li>
          </p></ul>
      </li>
      
      <li><b>Calculus, Linear Algebra</b>
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b>
          <p>You should know basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
      <li><b>Foundations of Machine Learning</b>
          <p>You should know the basics of machine learning.
             There are many introductions to ML, in webpage, book, and video form. One approachable introduction is Hal Daum&eacute;â€™s in-progress <a href="http://ciml.info"><i>A Course in Machine Learning</i></a>. Reading the first 5 chapters of that book would be good background. Knowing the first 7 chapters would be even better!</p>
      </li>
  </ul>

  <h2>Reference Texts</h2>
  <p>
    The following texts are useful, and all of them can be read free online.
  </p>
  <ul>
      <li>
          Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Language Processing (3rd ed. draft)</a>
    </li>
      <li>
          Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank">Natural Language Processing</a>
      </li>
      <li>
          Yoav Goldberg. <a href="http://u.cs.biu.ac.il/~yogo/nnlp.pdf" target="_blank">A Primer on Neural Network Models for Natural Language Processing</a>
      </li>
      <li>
          Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/" target="_blank">Deep Learning</a>
      </li>
    </ul>
    <p>
    If you have no background in neural networks, you might find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
         Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com" target="_blank">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank">Introduction to Deep Learning</a>
      </li>
  </ul>
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<br>
<h2>Content</h2>
<p>
The following courses and content I consider important theoretical background if you want to become involve in NLP with machine learning and/or deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>#</th>
    <th>Description</th>
    <th>Online Course</th>
  </tr>
  </thead>
  <tbody>
   
  <tr>
    <td>1</td>
    <td>Introduction to Machine Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Machine Learning: Reading first 7 chapters of "A Course on Machine Learning" (recommended by Stanford): <a href="ttp://ciml.info/">ttp://ciml.info/</a></li>
        <li>Machine Learning by Prof Andrew Ng (Stanford University), Coursera: <a href="https://www.coursera.org/learn/machine-learning/home/welcome target="_blank"">https://www.coursera.org/learn/machine-learning/home/welcome</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>2</td>
    <td>Introduction to Deep Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Introduction to Deep Learning by National Research University Higher School of Economics, Coursera: <a href="https://www.coursera.org/learn/intro-to-deep-learning" target="_blank">https://www.coursera.org/learn/intro-to-deep-learning</a></li>
      </ol>
    </td>
  </tr>
  
  <tr>
    <td>3</td>
    <td>Introduction to NLP
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>NLP with Python for Machine Learning Essential Training, LinkedIn Training: <a href="https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/welcome?u=37069596" target="_blank">https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training</a> [UFS login required]</li>
      </ol>
    </td>
  </tr>
  
  <tr>
    <td>4</td>
    <td>Advanced NLP
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Natural Language Processing by National Research University Higher School of Economics, Coursera: <a href="https://www.coursera.org/learn/language-processing" target="_blank">https://www.coursera.org/learn/language-processing</a></li>
      </ol>
    </td>
  </tr>
  
  <thead>
  <tr class="active">
    <th>Date</th>
    <th>Description</th>
    <th>Content Materials</th>
  </tr>
  </thead>
  
  <tr>
    <td>1</td>
    <td>Introduction to Regular Expressions and Text Normalization
    </td>
    <td>
      Suggested Reading:
      <ol>
        <li><a href=" https://web.stanford.edu/~jurafsky/slp3/2.pdf" target="_blank">Regular Expressions, Text Normalization, and Edit Distance</a> (textbook chapter 2)</li>
        <li><a href="https://dl.acm.org/doi/pdf/10.1145/1390749.1390763" target="_blank">Dey et al., (2009). Opinion Mining From Noisy Text Data - this paper covers text pre-processing paper</li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>2</td>
    <td>N-Grams modeling
	<br>
	      [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf" target="_blank">slides</a>]
		  [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pptx" target="_blank">powerpoint</a>]
    </td>
    <td>
      Suggested Reading:
      <ol>
        <li><a href=" https://web.stanford.edu/~jurafsky/slp3/3.pdf">Language modeling with N-Grams</a> (textbook chapter 3)</li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>3</td>
    <td>Text Classification and Sentiment Analysis
	<br>
		  NB [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf" target="_blank">slides</a>] 
	      NB [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pptx target="_blank">powerpoint</a>]
		  <br>
		  Sentiment [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_Sent.pdf" target="_blank">slides</a>]
		  Sentiment [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_Sent.pptx" target="_blank">powerpoint</a>]
    </td>
    <td>
      Suggested Reading:
      <ol>
        <li><a href=" https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank">Naive Bayes Classification and Sentiment</a> (textbook chapter 4)</li>
        <li><a href=" https://web.stanford.edu/~jurafsky/slp3/5.pdf" target="_blank">Logistic Regression</a> (textbook chapter 5)</li>		
		<li><a href="   https://collab.hpc.ufs.ac.za/index.php/s/pgfEzZ948XxaiF8" target="_blank">Sentiment Analysis</a> (summary by Dr E. KotzÃ©)</li>
      </ol>
    </td>
  </tr>
  
  <tr>
    <td>4</td>
    <td>Introduction and Word Vectors
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2020-lecture01-wordvecs1.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf" target="_blank">notes</a>]
      <br><br>
      Gensim word vectors example:
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/materials/Gensim.zip" target="_blank">python code</a>]
      [<a href="http://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html" target="_blank">preview</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
	      <li><a href=" https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Vector Semantics and Embeddings</a> (textbook chapter 6)</li>
          <li><a href=http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/>Word2Vec Tutorial - The Skip-Gram Model</a></li>
        <li>Mikolov et al., (2013a): <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a> (original word2vec paper)</li>
        <li>Mikolov et al., (2013b): <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
		<li>Le et al., (2014): <a href="https://arxiv.org/abs/1405.4053" target="_blank">Distributed Representations of Sentences and Documents</a> (original doc2vec/paragraphvector paper)</li>
      </ol>
    </td>
  </tr>

  https://arxiv.org/abs/1405.4053
  
    <tr class="warning">
    <td>Fri Jan 10</td>
    <td>Python review session
      <br>
      [<a href="readings/cs224n-python-review-20.pdf">slides</a>]
      [<a href="https://stanford.box.com/s/50i2a4w29adk0vfc075cuguv3tlu4t1x">video</a>]
      [<a href="readings/python-review-demo.py">code</a>]
    </td>
    <td>
      2:30 - 4:20pm<br>160-124 [<a href="https://campus-map.stanford.edu/">map</a>]
    </td>
  </tr>
  
  <tr>
    <td>Thu Jan 9</td>
    <td>Word Vectors 2 and Word Senses
      <br>
      [<a href="slides/cs224n-2020-lecture02-wordvecs2.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=68213ef7-af13-4e96-b154-ab25012bda54">video</a>]
      [<a href="readings/cs224n-2019-notes02-wordvecs2.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> (original GloVe paper)</li>
        <li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
        <li><a href="http://www.aclweb.org/anthology/D15-1036">Evaluation methods for unsupervised word embeddings</a></li>
      </ol>
      Additional Readings:
      <ol>
      	<li><a href="http://aclweb.org/anthology/Q16-1028">A Latent Variable Model Approach to PMI-based Word Embeddings</a></li>
      	<li><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a></li>
      	<li><a href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf">On the Dimensionality of Word Embedding.</a></li>
      </ol>
    </td>
  </tr>



  <tr>
    <td>Tue Jan 14</td>
    <td>Word Window Classification, Neural Networks, and PyTorch
      <br>
      [<a href="slides/cs224n-2020-lecture03-neuralnets.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=bd799d65-6da2-4bef-b075-ab25012bda8c">video</a>]
      [<a href="materials/ww_classifier.ipynb">code (notebook)</a>]
      [<a href="materials/ww_classifier.slides.html">code (html)</a>]
      <br>
      [<a href="readings/gradient-notes.pdf">matrix calculus notes</a>]
      <br>
      [<a href="readings/cs224n-2019-notes03-neuralnets.pdf">notes (lectures 3 and 4)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="readings/review-differential-calculus.pdf">Review of differential calculus</a></li>
      </ol>
      Additional Readings:
      <ol>
      	<li><a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf">Natural Language Processing (Almost) from Scratch</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Jan 16</td>
    <td>Matrix Calculus and Backpropagation
      <br>
      [<a href="slides/cs224n-2020-lecture04-neuralnets.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0661dbaa-3726-40f7-89d2-ab25012bdab6">video</a>]
      <br>
      [<a href="readings/cs224n-2019-notes03-neuralnets.pdf">notes (lectures 3 and 4)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://cs231n.github.io/neural-networks-1/">CS231n notes on network architectures</a></li>
        <li><a href="http://cs231n.github.io/optimization-2/">CS231n notes on backprop</a></li>
        <li><a href="http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf">Learning Representations by Backpropagating Errors</a></li>
        <li><a href="http://cs231n.stanford.edu/handouts/derivatives.pdf">Derivatives, Backpropagation, and Vectorization</a></li>
        <li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a></li>
      </ol>
      </td>
  </tr>

  <tr>
    <td>Tue Jan 21</td>
    <td>Linguistic Structure: Dependency Parsing
      <br>
      [<a href="slides/cs224n-2020-lecture05-dep-parsing.pdf">slides</a>]
      <br>
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0607b355-903d-4b4b-ad42-ab25012bdadc">video</a>]
	    [<a href="readings/cs224n-2019-notes04-dependencyparsing.pdf">notes</a>]
     </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.aclweb.org/anthology/W/W04/W04-0308.pdf">Incrementality in Deterministic Dependency Parsing</a></li>
        <li><a href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a></li>
        <li><a href="http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002">Dependency Parsing</a></li>
        <li><a href="https://arxiv.org/pdf/1603.06042.pdf">Globally Normalized Transition-Based Neural Networks</a></li>
        <li><a href="http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf">Universal Stanford Dependencies: A cross-linguistic typology</li>
        <li><a href="http://universaldependencies.org/">Universal Dependencies website</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Jan 23</td>
    <td>The probability of a sentence? Recurrent Neural Networks and Language Models
      <br>
      [<a href="slides/cs224n-2020-lecture06-rnnlm.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e021c806-efe5-42cb-9f40-ab25012bdb01">video</a>]
      <br>
      [<a href="readings/cs224n-2019-notes05-LM_RNN.pdf">notes (lectures 6 and 7)</a>]
    </td>

    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-gram Language Models</a> (textbook chapter)</li>
        <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (blog post overview)</li>
        <!-- <li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Recurrent Neural Networks Tutorial</a> (practical guide)</li> -->
        <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.1 and 10.2)</li>
	<li><a href="http://norvig.com/chomsky.html">On Chomsky and the Two Cultures of Statistical Learning</a>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Tue Jan 28</td>
    <td>Vanishing Gradients and Fancy RNNs
      <br>
      [<a href="slides/cs224n-2020-lecture07-fancy-rnn.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1420d65f-bf04-46b2-a8af-ab25012bdb27">video</a>]
      <br>
		  [<a href="readings/cs224n-2019-notes05-LM_RNN.pdf">notes (lectures 6 and 7)</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://www.deeplearningbook.org/contents/rnn.html">Sequence Modeling: Recurrent and Recursive Neural Nets</a> (Sections 10.3, 10.5, 10.7-10.12)</li>
        <li><a href="http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf">Learning long-term dependencies with gradient descent is difficult</a> (one of the original vanishing gradient papers)</li>
        <li><a href="https://arxiv.org/pdf/1211.5063.pdf">On the difficulty of training Recurrent Neural Networks</a> (proof of vanishing gradient problem)</li>
        <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html">Vanishing Gradients Jupyter Notebook</a> (demo for feedforward networks)</li>
        <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> (blog post overview)</li>
        <!-- <li><a href="https://arxiv.org/pdf/1504.00941.pdf">A simple way to initialize recurrent networks of rectified linear units</a></li> -->
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Jan 30</td>
    <td>Machine Translation, Seq2Seq and Attention
      <br>
      [<a href="slides/cs224n-2020-lecture08-nmt.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d951e69f-00b9-4f32-9856-ab25012bdb48">video</a>]
      [<a href="readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/syllabus.shtml">Statistical Machine Translation slides, CS224n 2015</a> (lectures 2/3/4)</li>
        <li><a href="https://www.cambridge.org/core/books/statistical-machine-translation/94EADF9F680558E13BE759997553CDE5">Statistical Machine Translation</a> (book by Philipp Koehn)</li>
        <li><a href="https://www.aclweb.org/anthology/P02-1040.pdf">BLEU</a> (original paper)</li>
        <li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</a></li>
        <li><a href="https://arxiv.org/pdf/1211.3711.pdf">Sequence Transduction with Recurrent Neural Networks</a> (early seq2seq speech recognition paper)</li>
        <li><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
        <li><a href="https://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a> (blog post overview)</li>
        <li><a href="https://arxiv.org/pdf/1703.03906.pdf">Massive Exploration of Neural Machine Translation Architectures</a> (practical advice for hyperparameter choices)</li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Tue Feb 4</td>
    <td>
      Practical Tips for Final Projects
      <br>
      [<a href="slides/cs224n-2020-lecture09-final-projects.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=4879f76a-c6df-4f8b-bba5-ab25012bdb6d">video</a>]
      [<a href="readings/final-project-practical-tips.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Feb 6</td>
    <td>Question Answering, the Default Final Project, and an introduction to Transformer architectures<br>
    [<a href="slides/cs224n-2020-lecture10-QA.pdf">slides</a>]
    [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d3863106-4554-4ee5-bdad-ab25012bdb93">video</a>]
    [<a href="readings/cs224n-2019-notes07-QA.pdf">notes</a>]
  </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://web.stanford.edu/class/cs224n/project/default-final-project-handout.pdf">Project Handout</a>
        </li>
        <li><a href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All You Need</a>
        </li>
        <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
        </li>
        <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer (Google AI blog post)</a>
        </li>
        <li><a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a></li>
        <li><a href="https://arxiv.org/pdf/1802.05751.pdf">Image Transformer</a></li>
        <li><a href="https://arxiv.org/pdf/1809.04281.pdf">Music Transformer: Generating music with long-term structure</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Tue Feb 11</td>
    <td>ConvNets for NLP <br>
    [<a href="slides/cs224n-2020-lecture11-convnets.pdf">slides</a>]
    [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=24790026-1fb3-4e53-bde0-ab25012bdbc2">video</a>]
    [<a href="readings/cs224n-2019-notes08-CNN.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/1408.5882.pdf">Convolutional Neural Networks for Sentence Classification</a></li>
        <li><a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li><a href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Feb 13</td>
    <td>Information from parts of words (Subword Models)
    <br>
    [<a href="slides/cs224n-2020-lecture12-subwords.pdf">slides</a>]
    [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=796bb4dc-fabf-4cc4-8b31-ab25012bdbec">video</a>]
    </td>
    <td>Suggested readings:
      <ol>
	    <li>
           <a href="https://arxiv.org/abs/1604.00788.pdf">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</a>
	    </li>
        <li>
          <a href="https://arxiv.org/pdf/1808.09943.pdf">Revisiting Character-Based Neural Machine Translation with Capacity and Compression</a>
        </li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Tue Feb 18</td>
    <td>Contextual Word Representations: BERT <i>(guest lecture by <a href="https://research.google/people/106320/">Jacob Devlin</a>)</i>
    <br>
    [<a href="slides/Jacob_Devlin_BERT.pdf">slides</a>]
    [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5a2ffb20-113f-4d8d-bb77-ab25012bdc20">video</a>]</td>
    <td>Suggested readings:
      <ol>
        <li>
          <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
        </li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Feb 20</td>
    <td>Modeling contexts of use: Contextual Representations and Pretraining. ELMo and BERT.
      <br>
      [<a href="slides/cs224n-2020-lecture14-contextual-representations.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=7f471f99-ebf5-4477-a700-ab25012bdc4a">video</a>]
    </td>
    <td>Suggested readings:
      <ol>
	       <li>
           <a href="https://arxiv.org/abs/1902.06006.pdf">Contextual Word Representations: A Contextual Introduction</a>.
         </li>
	 <li><a href="http://jalammar.github.io/illustrated-bert/">The
	 Illustrated BERT, ELMo, and co.</a>
	 </li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Tue Feb 25</td>
    <td>
      Natural Language Generation
      <br>
      [<a href="slides/cs224n-2020-lecture15-nlg.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=5b83adc9-8aed-49b4-82ba-ab25012bdc6c">video</a>]
    </td>
    <td>
    Suggested readings:
      <ol>
        <li>
        <a href="https://arxiv.org/abs/1904.09751.pdf">The Curious Case of Neural Text Degeneration</a>.
        </li>
        <li>
        <a href="https://arxiv.org/abs/1704.04368.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a>.
        </li>
        <li>
        <a href="https://arxiv.org/abs/1805.04833.pdf">Hierarchical Neural Story Generation</a>.
        </li>
        <li>
        <a href="https://arxiv.org/abs/1603.08023.pdf">How NOT To Evaluate Your Dialogue System</a>.
        </li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Feb 27</td>
    <td>Reference in Language and Coreference Resolution
      <br>
      [<a href="slides/cs224n-2019-lecture16-coref.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6b685084-c0f5-4a3e-ac1b-ab25012bdc9a">video</a>]
    </td>
    <td>Suggested readings:
      <ol>
	<li><a href="https://web.stanford.edu/~jurafsky/slp3/22.pdf">Coreference Resolution chapter of Jurafsky and Martin</a> </li>
        <li><a href="https://arxiv.org/pdf/1707.07045.pdf">End-to-end Neural Coreference Resolution</a> </li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Thu Mar 5</td>
    <td>
      Constituency Parsing and Tree Recursive Neural Networks
      <br>
      [<a href="slides/cs224n-2020-lecture18-TreeRNNs.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=42213860-265e-46c8-a16d-ab25012bdcf8">video</a>]
      [<a href="readings/cs224n-2019-notes09-RecursiveNN_constituencyparsing.pdf">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="http://www.aclweb.org/anthology/P13-1045">Parsing with Compositional Vector Grammars.</a></li>
        <li><a href="https://arxiv.org/pdf/1805.01052.pdf">Constituency Parsing with a Self-Attentive Encoder</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Fri Mar 6</td>
    <td>
      Virtual Office Hours with HuggingFace
      <br>
      [<a href="https://drive.google.com/file/d/16jNWI2HHEet6Jc8jD-FgpAkHNGZ13eHP/view?usp=sharing">video</a>] (requires Stanford login)
    </td>
    <td>
    </td>
  </tr>

  <tr>
    <td>Tue Mar 10</td>
    <td>Recent Advances in Low Resource Machine Translation <i>(guest lecture by <a href="https://ranzato.github.io">Marc'Aurelio Ranzato</a>)</i>
    <br>
    [<a href="slides/MarcAurelio_Ranzato_Low_Resource_MT.pdf">slides</a>]
    [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=7e2323b5-c044-4a48-9d37-ab7b00f1fbc9">video</a>]
    </td>
    <td></td>
  </tr>

  <tr>
    <td>Thu Mar 12</td>
    <td>
      Analysis and Interpretability of Neural NLP
      <br>
      [<a href="slides/cs224n-2020-lecture20-interpretability.pdf">slides</a>]
      [<a href="https://stanford-pilot.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e5926c50-b6b1-4339-bc06-ab7d006ffdea">video</a>]
    </td>
    <td></td>
  </tr>

  </tbody>
</table>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
