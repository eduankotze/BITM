# ðŸ§  Build the Self-Attention Mechanism in PyTorch from Scratch
By Jocelyne Smith <br>
May 2025

In this notebook, I explore and implement the self-attention mechanism that underpins transformer-based language models. Inspired by Damien Benvenisteâ€™s slide series, **"Build The Self-Attention in PyTorch From Scratch"**, I walk through the process of recreating the self-attention layer from the ground up using PyTorch. This hands-on exercise deepened my understanding of how decoder-only transformers process sequences and learn contextual relationships between tokens.

---

## ðŸ“˜ Key Concepts I Learned
- The core architecture of decoder-only transformer models
- How tokens are embedded and transformed into queries, keys, and values
- The mathematical intuition behind dot-product attention
- How self-attention selectively focuses on different parts of the input sequence
- How multi-head attention enables the model to capture diverse patterns in parallel
- How to implement each component in PyTorch, including attention weights, masking, and projection layers
---

## ðŸ“„ Supporting Materials

- [View the Slides (PDF)](./Build%20The%20Self-Attention%20in%20PyTorch%20From%20Scratch.pdf)

---
