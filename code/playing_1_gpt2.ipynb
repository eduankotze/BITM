{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2992d9-df6c-42f9-a582-4d4069ccaefa",
   "metadata": {},
   "source": [
    "# GPT in 60 Lines of NumPy\n",
    "\n",
    "**2024/08/12, 2025/03/06, 2025/03/26**\n",
    "\n",
    "* Source Code: https://github.com/jaymody/picoGPT/blob/main/README.md\n",
    "* Requirements: https://github.com/jaymody/picoGPT/blob/main/requirements.txt\n",
    "* Tutorial: https://jaykmody.com/blog/gpt-from-scratch/\n",
    "* Other sources:\n",
    "    * **Jay Alammar**: https://jalammar.github.io/illustrated-gpt2/\n",
    "    * **OpenAI** gpt-2 implementation: https://github.com/openai/gpt-2/\n",
    "    * **Academic Paper**: https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b40127ca-ed9c-4016-ae6e-e1b7b1cda899",
   "metadata": {},
   "source": [
    "pip install tqdm==4.64.0\n",
    "pip install fire==0.5.0\n",
    "pip install regex==2017.4.5\n",
    "pip install -U \"jax[cuda12]\" # NVIDIA GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ddcf7e-04fa-4885-9102-719460fdbafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6acf31-5362-4f57-833a-f7b838595b86",
   "metadata": {},
   "source": [
    "## What is a GPT?\n",
    "\n",
    "GPT stands for **Generative Pre-trained Transformer**. It's a type of neural network architecture based on the Transformer. \n",
    "\n",
    "    Generative: A GPT generates text.\n",
    "    Pre-trained: A GPT is trained on lots of text from books, the internet, etc ...\n",
    "    Transformer: A GPT is a decoder-only transformer neural network.\n",
    "\n",
    "Fundamentally, a GPT **generates text** given a **prompt**. Even with this very simple API (input = text, output = text), a well-trained GPT can do some pretty awesome stuff like write your emails, summarize a book, give you instagram caption ideas, explain black holes to a 5 year old, code in SQL, and even write your will."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a088dc4-2d1d-4ea9-8849-c720ea128479",
   "metadata": {},
   "source": [
    "## Where to download the model from?\n",
    "\n",
    "* From the original <mark>openai/gpt-2 github!</mark>\n",
    "* First, read: https://github.com/openai/gpt-2/blob/master/README.md\n",
    "* Second, download from: https://github.com/openai/gpt-2/blob/master/download_model.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a69cc5-0e89-45db-ad15-924105c82a3a",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8e5f4-4734-4070-ab10-c8de2af1f783",
   "metadata": {},
   "source": [
    "# Tutorial (30 January 2023)\n",
    "\n",
    "https://jaykmody.com/blog/gpt-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74eb7de-ee46-421f-9d1a-7f191a27eec8",
   "metadata": {},
   "source": [
    "## Input / Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769a98e-63eb-4d31-ada7-321863e964d5",
   "metadata": {},
   "source": [
    "<font color=\"red\">The function signature for a GPT looks roughly like this:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f9a7c6-ec5c-440c-8447-f735862b2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt(inputs: list[int]) -> list[list[float]]:\n",
    "    # inputs has shape [n_seq]\n",
    "    # output has shape [n_seq, n_vocab]\n",
    "    # neural network magic happens here\n",
    "    \n",
    "    output = 0 \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8a981-6e5a-4009-b0cd-5a27b556bebe",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "The input is some text represented by a sequence of integers that map to tokens in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366399f0-f5ea-48b7-9b3c-f78c852663d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers represent tokens in our text, for example:\n",
    "# text   = \"not all heroes wear capes\":\n",
    "# tokens = \"not\"  \"all\" \"heroes\" \"wear\" \"capes\"\n",
    "inputs =   [1,     0,    2,      4,     6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844c649-3bb6-42d2-995c-6884de600056",
   "metadata": {},
   "source": [
    "Tokens are sub-pieces of the text, which are produced using some kind of tokenizer. We can map tokens to integers using a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af072d2c-83a5-47fa-9a79-547e78f95f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.token_to_id = {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = text.split()  # Tokenize on whitespace\n",
    "        return [self.token_to_id[token] for token in tokens if token in self.token_to_id]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.vocab[i] for i in ids]  # Convert IDs back to tokens\n",
    "        return \" \".join(tokens)  # Join tokens with a space to form the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144d5c1f-60ac-4a43-ab1e-23540d79b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 4]\n",
      "['not', 'all', 'heroes', 'wear']\n",
      "not all heroes wear\n"
     ]
    }
   ],
   "source": [
    "# the index of a token in the vocab represents the integer id for that token\n",
    "# i.e. the integer id for \"heroes\" would be 2, since vocab[2] = \"heroes\"\n",
    "vocab = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "\n",
    "# a pretend tokenizer that tokenizes on whitespace\n",
    "tokenizer = WhitespaceTokenizer(vocab)\n",
    "\n",
    "# the encode() method converts a str -> list[int]\n",
    "ids = tokenizer.encode(\"not all heroes wear\") # ids = [1, 0, 2, 4]\n",
    "print(ids)\n",
    "\n",
    "# we can see what the actual tokens are via our vocab mapping\n",
    "tokens = [tokenizer.vocab[i] for i in ids] # tokens = [\"not\", \"all\", \"heroes\", \"wear\"]\n",
    "print(tokens)\n",
    "\n",
    "# the decode() method converts back a list[int] -> str\n",
    "text = tokenizer.decode(ids) # text = \"not all heroes wear\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cae06-1a3d-46f8-84a0-601da0fc7037",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The output is a 2D array, where `output[i][j]` is the model's predicted probability that the token at `vocab[j]` is the next token `inputs[i+1]`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2059faba-85e9-4d09-9c0e-628801701676",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "\n",
    "text_inputs = [\"Not\", \"all\",  \"heroes\", \"wear\"]\n",
    "inputs = [1, 0, 2, 4] # \"not\" \"all\" \"heroes\" \"wear\"\n",
    "output = gpt(inputs)\n",
    "\n",
    "# some hypothetical output results\n",
    "# =================================\n",
    "\n",
    "#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "# output[0] =  [0.75    0.1     0.0       0.15    0.0   0.0    0.0  ]\n",
    "# given just \"not\", the model predicts the word \"all\" with the highest probability\n",
    "\n",
    "#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "# output[1] =  [0.0     0.0      0.8     0.1    0.0    0.0   0.1  ]\n",
    "# given the sequence [\"not\", \"all\"], the model predicts the word \"heroes\" with the highest probability\n",
    "\n",
    "#              [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "# output[-1] = [0.0     0.0     0.0     0.1     0.0    0.05  0.85  ]\n",
    "# given the whole sequence [\"not\", \"all\", \"heroes\", \"wear\"], the model predicts the word \"capes\" with the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8a67c-4b9e-4419-ad35-6c6290a61466",
   "metadata": {},
   "source": [
    "To get a **next token prediction** for the whole sequence, we simply take the token with the highest probability in `output[-1]`:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3d64922-cc9e-4d73-8c85-f1b09bb52016",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "vocab = [\"all\", \"not\", \"heroes\", \"the\", \"wear\", \".\", \"capes\"]\n",
    "inputs = [1, 0, 2, 4] # \"not\" \"all\" \"heroes\" \"wear\"\n",
    "output = gpt(inputs)\n",
    "\n",
    "next_token_id = np.argmax(output[-1]) # next_token_id = 6\n",
    "next_token = vocab[next_token_id] # next_token = \"capes\"\n",
    "\n",
    "# this code will not work, since we did not implement gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0b12e-4a8b-49ed-9a2f-b1936dd16fdc",
   "metadata": {},
   "source": [
    "Taking the token with the highest probability as our prediction is known as `greedy decoding` or `greedy sampling`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d2957-bc26-4f54-911f-58f2949b368b",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "\n",
    "### Autoregressive\n",
    "\n",
    "We can generate full sentences by iteratively getting the next token prediction from our model. At each iteration, we append the predicted token back into the input.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "We can introduce some stochasticity (randomness) to our generations by sampling from the probability distribution instead of being greedy.\n",
    "\n",
    "### Training\n",
    "\n",
    "We train a GPT like any other neural network, using **gradient descent** with respect to some **loss function**. In the case of a GPT, we take the **cross entropy loss over the language modeling task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706fd7c5-833f-45ef-bf56-e08e1486a037",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd0a20-ad3b-4867-b2d6-28218a6f476b",
   "metadata": {},
   "source": [
    "## Gpt-2 code\n",
    "\n",
    "* Downloaded from: https://github.com/jaymody/picoGPT/blob/main/gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc42c17-5693-45c6-a4ce-534a54122585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def layer_norm(x, g, b, eps: float = 1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
    "    return g * x + b  # scale and offset with gamma/beta params\n",
    "\n",
    "\n",
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    return x @ w + b\n",
    "\n",
    "\n",
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n",
    "\n",
    "\n",
    "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # qkv projection\n",
    "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "    # split into qkv\n",
    "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "\n",
    "    # split into heads\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "    # perform attention over each head\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # merge heads\n",
    "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "\n",
    "    # out projection\n",
    "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    # token(wte) + positional embeddings(wpe)\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # projection to vocab\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]\n",
    "\n",
    "\n",
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad25814-355e-4523-970b-9a87ea2fdae5",
   "metadata": {},
   "source": [
    "## Load Model (GPT2-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592a0d56-3a87-44d2-ac27-b2a092df31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import get_encoder\n",
    "from utils import load_encoder_hparams_and_params, download_gpt2_files"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f8a176d-1e5c-40ce-8d35-e19267ca0572",
   "metadata": {},
   "source": [
    "###from utils.py###\n",
    "\n",
    "def load_encoder_hparams_and_params(model_size, models_dir):\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    if not tf_ckpt_path:  # download files if necessary\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        download_gpt2_files(model_size, model_dir)\n",
    "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "    encoder = get_encoder(model_size, models_dir)\n",
    "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
    "\n",
    "    return encoder, hparams, params"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9600ae2-7e71-42f2-b2b9-ffeb927973b6",
   "metadata": {},
   "source": [
    "###from utils.py###\n",
    "\n",
    "def download_gpt2_files(model_size, model_dir):\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "    for filename in [\n",
    "        \"checkpoint\",\n",
    "        \"encoder.json\",\n",
    "        \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\",\n",
    "        \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\",\n",
    "        \"vocab.bpe\",\n",
    "    ]:\n",
    "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(\n",
    "                ncols=100,\n",
    "                desc=\"Fetching \" + filename,\n",
    "                total=file_size,\n",
    "                unit_scale=True,\n",
    "                unit=\"b\",\n",
    "            ) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0515375-c6be-40ac-a8de-357eefebd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kb [00:00, 1.00Mb/s]                                                       \n",
      "Fetching encoder.json: 1.04Mb [00:01, 783kb/s]                                                      \n",
      "Fetching hparams.json: 1.00kb [00:00, 1.01Mb/s]                                                     \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mb [04:19, 1.92Mb/s]                                    \n",
      "Fetching model.ckpt.index: 6.00kb [00:00, ?b/s]                                                     \n",
      "Fetching model.ckpt.meta: 472kb [00:01, 464kb/s]                                                    \n",
      "Fetching vocab.bpe: 457kb [00:01, 450kb/s]                                                          \n"
     ]
    }
   ],
   "source": [
    "download_gpt2_files(\"124M\", \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f5628-f2b2-472a-b99e-b0a2b9438ddf",
   "metadata": {},
   "source": [
    "**create subdirectory /124M in models, move the files to that subdirectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af786386-143d-4305-9bf3-686d1d8904a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoder, hparams, and params from the released open-ai gpt-2 files\n",
    "# [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "\n",
    "model_size=\"124M\"\n",
    "models_dir=\"models\"\n",
    "encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398e51dd-b305-4ef2-a5de-da65eee7300e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4acff-50e9-4bfd-a695-3325593bc643",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "`params` is a nested json dictionary that hold the trained weights of our model. The leaf nodes of the json are NumPy arrays. If we print params, replacing the arrays with their shapes, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65a1060-4664-45b7-a8a5-daf394e5ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "def shape_tree(d):\n",
    "    if isinstance(d, np.ndarray):\n",
    "        return list(d.shape)\n",
    "    elif isinstance(d, list):\n",
    "        return [shape_tree(v) for v in d]\n",
    "    elif isinstance(d, dict):\n",
    "        return {k: shape_tree(v) for k, v in d.items()}\n",
    "    else:\n",
    "        ValueError(\"uh oh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b70c4c3-a66d-4eb2-9be4-7f9b625987ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blocks': [{'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}},\n",
      "            {'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "                      'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      "             'ln_1': {'b': [768], 'g': [768]},\n",
      "             'ln_2': {'b': [768], 'g': [768]},\n",
      "             'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "                     'c_proj': {'b': [768], 'w': [3072, 768]}}}],\n",
      " 'ln_f': {'b': [768], 'g': [768]},\n",
      " 'wpe': [1024, 768],\n",
      " 'wte': [50257, 768]}\n"
     ]
    }
   ],
   "source": [
    "pprint(shape_tree(params)) # 12 layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b4b9de4-45f5-4b49-bd59-88c8de9c9100",
   "metadata": {},
   "source": [
    "wpe (Position Embeddings): The weights for the position embeddings, which encode the position of each token in the sequence.\n",
    "\n",
    "    Shape [1024, 768] suggests a maximum sequence length of 1024 with an embedding size of 768.\n",
    "\n",
    "wte (Token Embeddings): The weights for the token embeddings, which map the input tokens to dense vectors.\n",
    "\n",
    "    Shape [50257, 768] indicates a vocabulary size of 50,257 with an embedding size of 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c165144-8525-4301-9328-9754455395db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "          'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      " 'ln_1': {'b': [768], 'g': [768]},\n",
      " 'ln_2': {'b': [768], 'g': [768]},\n",
      " 'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "         'c_proj': {'b': [768], 'w': [3072, 768]}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(shape_tree(params['blocks'][0])) # 1st layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af90e739-d288-4496-a242-b78239eee12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attn': {'c_attn': {'b': [2304], 'w': [768, 2304]},\n",
      "          'c_proj': {'b': [768], 'w': [768, 768]}},\n",
      " 'ln_1': {'b': [768], 'g': [768]},\n",
      " 'ln_2': {'b': [768], 'g': [768]},\n",
      " 'mlp': {'c_fc': {'b': [3072], 'w': [768, 3072]},\n",
      "         'c_proj': {'b': [768], 'w': [3072, 768]}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(shape_tree(params['blocks'][11])) # 12, last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f249fa-8286-4dea-88b4-ddbffe0caa77",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "See: https://github.com/jaymody/picoGPT/blob/main/gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e76ca3b-58b5-40f7-9df5-6eba6c4320ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the size of the vocabulary\n",
    "len(encoder.decoder)\n",
    "# 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d5ce73-0177-4a41-b165-9ee9aed54c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36235, 39141, 18765, 1143, 326, 9061, 561, 530, 1110, 1716]\n"
     ]
    }
   ],
   "source": [
    "# encode the input string using the BPE tokenizer\n",
    "prompt = \"Alan Turing theorized that computers would one day become\"\n",
    "\n",
    "input_ids = encoder.encode(prompt)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d3a413-4b4c-4908-bffa-beee92e66376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan',\n",
       " 'ĠTuring',\n",
       " 'Ġtheor',\n",
       " 'ized',\n",
       " 'Ġthat',\n",
       " 'Ġcomputers',\n",
       " 'Ġwould',\n",
       " 'Ġone',\n",
       " 'Ġday',\n",
       " 'Ġbecome']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoder.decoder[i] for i in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82868c29-a9a7-4f01-910c-81fdebab0db8",
   "metadata": {},
   "source": [
    "Notice, sometimes our tokens are words (e.g. `Alan`), sometimes they are words but with a space in front of them (e.g. `Ġthat`, the `Ġ represents a space`), sometimes there are part of a word (e.g. theorized is split into `Ġtheor` and `ized`), and sometimes they are punctuation (e.g. .)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e94063-ef9e-4e50-9970-c002f820e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alan', 'Turing', 'theorized', 'that', 'computers', 'would', 'one', 'day', 'become']\n"
     ]
    }
   ],
   "source": [
    "# we can get our words back again\n",
    "words = encoder.decode(input_ids)\n",
    "print(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c869684f-a398-43d7-a210-69e8ba707371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "# make sure we are not surpassing the max sequence length of our model\n",
    "n_tokens_to_generate = 40\n",
    "\n",
    "print(hparams[\"n_ctx\"])\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6e23f9-3f1b-456c-9b20-0a0ebf142d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 13.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate output ids (using CPU)\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f2de198-f582-4c1e-84fd-feb775be518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the ids back into a string\n",
    "output_text = encoder.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6950d9e7-67b0-46c4-b858-7f53df3899b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan Turing theorized that computers would one day become... the most powerful machines on the planet.\n",
      "\n",
      "The computer is a machine that can perform complex calculations, and it can perform these calculations in a way that is very similar to the human brain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the generated text.\n",
    "print(prompt + '...' + output_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24eae4d7-39e5-448f-8978-d5cfd913a73b",
   "metadata": {},
   "source": [
    "We managed to generate next tokens that are gramatically and syntactically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d410e-e1bf-4a25-9b48-156e3c6e18ba",
   "metadata": {},
   "source": [
    "## Final paper about GPT-2 on OpenAI.\n",
    "\n",
    "https://openai.com/index/gpt-2-1-5b-release/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.10.1",
   "language": "python",
   "name": "tf2.10.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
