<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
  <title>CSIA7915</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>

<!-- Header -->
<div id="header" style="text-align:center">
  <h1>Natural Language Processing (CSIA7915) - 2023</h1><br>
  <h4>Adopted from <a href="http://web.stanford.edu/class/cs124/" target="_blank">CS124</a> and
  <a href="http://web.stanford.edu/class/cs224n/" target="_blank">CS224n</a> (Stanford University),
  <a href="https://www.cis.upenn.edu/~myatskar/teaching/cis530_fa22//lectures.html" target="_blank">CIS530 Computational Linguistics</a>, and  
  <a href="https://www.uantwerpen.be/en/study/programmes/all-programmes/digital-text-analysis/" target="_blank">Digital Text Analytics</a> (University of Antwerp)</h4>    
  <b>Updated: 2023/06/02</b>
  <div style="clear:both;"></div>
</div>

<!-- Content -->
<div class="container sec" id="content">

  <h2>Prerequisites</h2>
  <ul>
      <li><b>Proficiency in Python and Machine Learning</b>
          <p>You should know the basics of Python 3 and machine learning (i.e. using Numpy, Pandas, and scikit-learn).</p>
          <p><u>The following resources are recommended if you new to Python Programming:</u></p>
          <ul><p>
              <li>Kaggle introduction to Python: <a href="https://www.kaggle.com/learn/python" target="_blank">Kaggle - Python</a> - Good introduction if you are familiar with programming.</li>		
		  <li>Google introduction to Python (2022) [<font color="red">highly recommended</font>]: 
			  <a href="https://developers.google.com/edu/python" target="_blank">Googleâ€™s Python Class</a>
			  - If you know how to program but are not proficient in Python, 
			  I would recommend going through <b>Googleâ€™s Python Class</b> for people who already know how to code, taught by Nick Parlante.</li>	
          </p></ul>
          
          <p><u>The following resources are recommended if you new to Machine Learning:</u></p>
          <ul><p>
              <li>Kaggle introduction to ML: <a href="https://www.kaggle.com/learn/intro-to-machine-learning" target="_blank">Kaggle - Intro to Machine Learning</a> - Learn the core ideas 
		      in machine learning, and build your first models.</li>				  
		  <li>Google introduction to ML (2022) [<font color="red">highly recommended</font>]: 
			  <a href="https://developers.google.com/machine-learning/crash-course/prereqs-and-prework" target="_blank">
				  Google's Machine Learning Crash Course with TensorFlow APIs</a>
			  - I would recommend going through <b>Googleâ€™s ML Crash Course</b> if you have little
			    machine learning background or you would like a more current and complete understanding.</li>	
          </p></ul>
	      
          <p><u>Useful Python Resources:</u></p>
          <ul><p>
              <li>NLTK: <a href="https://www.nltk.org/" target="_blank">https://www.nltk.org/</a> - Natural Langauge Toolkit (Bird, 2009)</li>
	      <li>Pandas: <a href="https://pandas.pydata.org/" target="_blank">https://pandas.pydata.org/</a>
		      - Pandas is a package for simultaneously working with different types of labeled data. 
	      <li>Gensim: <a href="https://pypi.org/project/gensim/" target="_blank">https://pypi.org/project/gensim/</a>
		      - Gensim is a library for topic modelling, document indexing and similarity retrieval with large corpora.</li>
	      <li>Scikit-learn: <a href="https://scikit-learn.org/stable/" target="_blank">https://scikit-learn.org/stable/</a>
		  - Scikit-learn is a Python library for implementing machine learning algorithms. </li>
              <li>tf.keras: <a href="https://keras.io/examples/nlp/" target="_blank">https://keras.io/examples/nlp/</a>	
		  - tf.keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. 
		  It was developed with a focus on enabling fast experimentation and is a beginner-friendly toolkit for working with neural networks.</li>
              <li>Transformers: <a href="https://huggingface.co/docs/transformers/index" target="_blank">https://huggingface.co/docs/transformers/index</a>	
		  - ðŸ¤— Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. 
   	         Using pretrained models can reduce your compute costs, and save you the time and resources required to train a model from scratch.
		 These models support common NLP tasks such as:  text classification, named entity recognition, question answering, language modeling, summarization, translation, 
		 multiple choice, and text generation.</li>
          </p></ul>
      </li>
      
      <li><b>Calculus, Linear Algebra</b>
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b>
          <p>You should know basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
  </ul>

  <h2>Reference Texts</h2>
  <p>
    The following texts are useful for theoretical background, and all of them can be read free online:
  </p>
  <ul>
      <li>Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Language Processing (3rd ed. draft)</a> [free downloads]</li>
      <li>Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank">Natural Language Processing</a> [free download]</li>
      <li>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/" target="_blank">Deep Learning</a> [free downloads]</li>
  </ul>
    <p>
    The following hands-on books are useful for NLP, machine and deep learning:
    </p>
    <ul>
      <li>Tunstall, Von Werra and Wolf. <a href="https://transformersbook.com/" target="_blank">Natural Language Processing with Transformers. 1st edition. 2022. O'Reilly</a></li>
      <li>Sebastian Raschka and Vahid Mirjalili. <a href="https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn" target="_blank">Python Machine Learning. 4th edition. 2022. Packt</a></li>
      <li>Raaijmakers. <a href="https://www.manning.com/books/deep-learning-for-natural-language-processing" target="_blank">Deep Learning for Natural Language Processing. 1st edition. 2022.</a></li>
      <li>Cholet. <a href="https://www.manning.com/books/deep-learning-with-python-second-edition" target="_blank">Deep Learning with Python. 2nd edition. 2021</a></li>
      <li>Azunre. <a href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" target="_blank">Transfer Learning for Natural Language Processing. 1st edition. 2021</a></li>
      <li>Lane, Howard and Hapke. <a href="https://www.manning.com/books/natural-language-processing-in-action" target="_blank">Natural Language Processing in Action. 1st edition. 2019</a></li>	    
  </ul>
    <p>
    If you have no background in neural networks, you might find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
      Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com" target="_blank">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank">Introduction to Deep Learning</a>
      </li>
  </ul>  


  <h2>Online Material</h2>
  <p>
    The following online resources will help you stay informed with the field of NLP:
  </p>
  <ul>
      <li><a href="https://paperswithcode.com/methods/area/natural-language-processing" target="_blank">Papers with Code</a></li>
      <li><a href="https://madewithml.com/topics/natural-language-processing/" target="_blank">Made with ML</a></li>
      <li><a href="https://nlpprogress.com/" target="_blank">NLP Progress</a></li>	  
      <li><a href="https://ruder.io/nlp-news/" target="_blank">NLP News by Sebastian Ruder</a></li>		  
  </ul>

</div>
	
<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Online Courses</h2>
<p>
The following courses I found useful in learning about machine learning and deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>#</th>
    <th>Description</th>
    <th>Online Course</th>
  </tr>
  </thead>
  <tbody>
   
  <tr>
    <td>1</td>
    <td>Machine Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Coursera: <a href="https://www.deeplearning.ai/courses/machine-learning-specialization/" target="_blank">Machine Learning by Prof Andrew Ng (Stanford University)</a>  
		- This is a very clearly-taught free online course which covers the basics of machine learning from an algorithmic perspective. 
		It assumes undergraduate-level understanding of mathematics and programming, and steps through detailed considerations of some of the most important machine learning algorithms. 
		Homework assignments, which are algorithmically graded, have you actually implement some of these models yourself.</li>
      </ol>
      Suggested Online Tutorials:
      <ol>
        <li><b>Kaggle.com</b>: <a href="https://www.kaggle.com/learn/intro-to-machine-learning" target="_blank">Intro to Machine Learning</a> (covers how models work, model validation, underfitting, overfitting, random forest)</li>
        <li><b>Kaggle.com</b>: <a href="https://www.kaggle.com/learn/intermediate-machine-learning" target="_blank">Intermediate Machine Learning</a> (covers missing values, categorical variables, pipelines, cross-validation, XGBoost, data leakage)</li>
        <li><b>YouTube.com</b>: <a href="https://www.youtube.com/watch?v=pHiMN_gy9mk" target="_blank">Machine Learning Roadmap (95% valid for 2023)</a> (highly recommended!)</li>	      
      </ol>
    </td>	
  </tr>

  <tr>
    <td>2</td>
    <td>Deep Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Coursera: <a href="https://www.deeplearning.ai/courses/deep-learning-specialization/" target="_blank">Deep Learning by Prof Andrew Ng (Stanford University)</a>  
		- This is a very clearly-taught free online course which covers the basics of deep learning and consists of 5 courses. 
	</li>
      </ol>
    </td>
  </tr>
	  

  <tr>
    <td>3</td>
    <td>Natural Language Processing
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Coursera: <a href="https://www.deeplearning.ai/courses/natural-language-processing-specialization/" target="_blank">Natural Language Processing by Prof Andrew Ng (Stanford University)</a>  
		- This is a very clearly-taught free online course which covers the basics of natural language processing and consists of 4 courses.
	</li>	      
     </ol>
    </td>
  </tr>	  
  
 </tbody>
</table>
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Content</h2>
<p>
The learning material I consider important theoretical background if you want to become involve in NLP with machine learning and/or deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>Week</th>
    <th>Description</th>
    <th>Learning Materials</th>
  </tr>
  </thead>
  
  <tr>
    <td>Week 1 <b>27/02/2023</b> </td> 
    <td>Introduction to the module
    </td>
    </td>
    <td>
      Rehash numpy, pandas, matplotlib. <br>    
      Suggested Reading: Jake VanderPlas, Python Data Science Handbook. Essential Tools for Working with Data. O'Reilly Media (2016):
      <ol>
        <li>numpy: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html" target="_blank">Introduction to NumPy</a></li>
        <li>pandas: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html" target="_blank">Data Manipulation with Pandas</a></li>
	<li>matplotlib: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html" target="_blank">Visualization with Matplotlib</a></li>
      </ol>
     	    
      Machine Learning Fundamentals (with scikit-learn):
      <ol>
      <li><a href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank">Cross-validation: evaluating estimator performance (3.1)</a></li>
      <li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html" target="_blank">Visualizing cross-validation</a></li>	      
      <li><a href="https://scikit-learn.org/stable/modules/grid_search.html" target="_blank">Tuning the hyper-parameters of an estimator (3.2)</a></li>
      <li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank">Metrics and scoring: quantifying the quality of predictions (3.3)</a></li>	      
      <li><a href="https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve" target="_blank">Validation curve (3.4)</a></li>	      
      <li><a href="https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve" target="_blank">Learning curve (3.4)</a></li>	      
      <li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html" target="_blank">Learning curve (example)</a></li>	      
      </ol>
    </td>
  </tr>

  <tr>
    <td>Week 2 <b>06/03/2023</b> </td>
    <td>Text Preprocessing and Language Modelling
    <td>
	  Suggested Reading:
      <ol>
        <li>Wikipedia (n.d.): <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">Brief history of Natural Language Processing</a></li>
      </ol>
	  Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href=" https://web.stanford.edu/~jurafsky/slp3/2.pdf" target="_blank">Chapter 2 - Regular Expressions, Text Normalization, and Edit Distance</a> (textbook chapter)</li>
      </ol>
	  
      Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter). 
		<font color="red">Pages 1-16 (plus section 3.7 "Huge Language Models and Stupid Backoff")</font></li>
      </ol>
	    
      Python examples:
      <ol>
	    <li>Gensim: <a href="https://radimrehurek.com/gensim/parsing/preprocessing.html" target="_blank">Preprocessing raw text</a></li>
	    <li>NLTK: <a href="https://www.kaggle.com/awadhi123/text-preprocessing-using-nltk/" target="_blank">Text preprocessing using NLTK in Python</a></li>
	    <li>scikit-learn (feature extraction): <a href="https://scikit-learn.org/stable/modules/feature_extraction.html" target="blank">Text Feature Extraction (BoW, TFIDF, n-grams)</a></li>
      </ol>	 
	  
    </td>
  </tr>


  <tr>
    <td>Week 3 <b>13/03/2023</b> </td>
    <td>Text Classification and Sentiment Analysis
	<br>
		  NB [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf" target="_blank">slides</a>] <br>
		  Sentiment [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_Sent.pdf" target="_blank">slides</a>] <br>
	          Lexicons [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/20_Affect_Jan_18_20201.pdf" target="_blank">slides</a>]
    </td>
    <td>
      Suggested Reading - Naive Bayes and Text Classification:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank">Chapter 4 - Naive Bayes Classification and Sentiment Classification</a> (textbook chapter). 
		<font color="red">pages 1-14 plus page 18, sections 4.1 through 4.8 and 4.10</font></li>
	<li>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. (2002): <a href=http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf" target="_blank">Thumbs up? Sentiment Classification using Machine Learning Techniques.</a> EMNLP 2002, pages 79-86</li>	      
      </ol>
	  
      Suggested Reading - Logistic Regression:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf" target="_blank">Chapter 5 - Logistic Regression</a> (textbook chapter). 
		<font color="red">pages 1-14. Pages 18-19 may also be useful! </font></li>		
      </ol>	 

      Suggested Reading - Lexicons:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/25.pdf" target="_blank">Chapter 25 - Lexicons for Sentiment, Affect, and Connotation</a> (textbook chapter).</li>		
      </ol>	
		
      Pipelines in sklearn:
      <ol>
        <li><a href="https://scikit-learn.org/stable/modules/compose.html" target="_blank">scikit-learn (pipeline)</a></li>
	<li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_text_feature_extraction.html">Sample pipeline for text feature extraction and evaluation</a></li>
	<li><a href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">Working With Text Data (features, pipeline, grid search)</a></li>		
      </ol>
		
      Ensembles in sklearn:
      <ol>
        <li><a href="https://scikit-learn.org/stable/modules/ensemble.html" target="_blank">Ensemble methods</a></li>
	<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" target="_blank">VotingClassifier</a></li>
	<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html" target="_blank">StackingClassifier</a></li>		
      </ol>
		
      <u>Python Example: Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	<ol>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb" target="_blank">Chapter 8 - Applying Machine Learning To Sentiment Analysis</a></li>
	</ol>
	  
     </td>
  </tr>
 
		
  <tr class="warning">
    <td><b>20/04/2023</b></td>
    <td>
      No class. Public Holiday
    </td>
    <td>
    </td>
  </tr>
		
		
		
  <tr>
    <td>Week 4 <b>27/03/2023</b> </td>
    <td>Word Vectors
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture01-wordvecs1.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf" target="_blank">notes (lecture 1)</a>]
    </td>
    <td>
		
      Suggested Readings (word2vec):
      <ol>
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Chapter 6 - Vector Semantics and Embeddings</a> (textbook chapter). 
		<font color="red">pages 1-7, 17-26</font></li>
        <li>Mikolov et al (2013a): <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a> (original <b>word2vec</b> paper)</li>
        <li>Mikolov et al (2013b): <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
      </ol>		

      Suggested Readings (doc2vec and fasttext):
      <ol>
	<li>Le et al (2014): <a href="https://arxiv.org/abs/1405.4053" target="_blank">Distributed Representations of Sentences and Documents</a> (original <b>doc2vec</b> paper)</li>
        <li>Bonanowki et al (2017): <a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with Subword Information</a> (original <b>FastText</b> paper)</li>		
      </ol>
		
      Official Documentation:
      <ol>
        <li>Word2vec: <a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank">Word2vec embeddings</a> </li>
        <li>Doc2vec:  <a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank">Doc2vec paragraph embeddings</a> </li>
        <li>FastText: <a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank">FastText model</a> </li>			
      </ol>
		
      Suggested Tutorials:
      <ol>
        <li><a href="http://jalammar.github.io/illustrated-word2vec/" target="_blank">The Illustrated Word2Vec</a> (tutorial by Jay Alammar)</li>		
        <li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank">Word2Vec Tutorial - The Skip-Gram Model</a> (tutorial by McCormick)</li>
      </ol>	
		
      Gensim Learning-Oriented Lessons:
      <ol>	
	<li><b>Gensim</b> Word2Vec Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html" target="_blank">Word2Vec Model</a></li>
	<li><b>Gensim</b> Doc2Vec Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html" target="_blank">Doc2Vec Model</a></li>		
	<li><b>Gensim</b> FastText Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html" target="_blank">FastText Model</a></li>
      </ol>
    </td>
  </tr>
		
  <tr>
    <td>Week 5 <b>03/04/2023</b> </td>
    <td>Pre-Trained Word Vectors (glove)
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf" target="_blank">notes (lecture 2)</a>]<br>
	  [<a href="http://nlp.stanford.edu/projects/glove/" target="_blank">glove pretrained</a>]
      <br><br>
      Gensim word vectors example (GloVe):
      [<a href="http://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html" target="_blank">preview</a>]
    </td>
    <td>
		
    Suggested Readings:
      <ol>
        <li>Pennington et al (2014): <a href="http://aclweb.org/anthology/D/D14/D14-1162.pdf" target="_blank">GloVe: Global Vectors for Word Representation</a> (original <b>GloVe</b> paper)</li>
        <li>Levy et al (2015): <a href="http://www.aclweb.org/anthology/Q15-1016" target="_blank">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
	<li>Mikolov et al (2018): <a href="https://arxiv.org/abs/1712.09405" target="_blank">Advances in Pre-Training Distributed Word Representations</a></li>		
      </ol>

    Official Documentation:
    <ol>
        <li>Keras.io: <a href="https://keras.io/api/layers/core_layers/embedding/" target="_blank">Embedding layer.</a></li>
	<li>Gensim: <a href="https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow" target="_blank">Using Gensim Embeddings with Keras and Tensorflow.</a></li>	
    </ol>
	
    Python examples of word embeddings:
      <ol>	  
	    <li><b>Keras.IO: </b><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank">Using pre-trained word embeddings in a Keras model</a> (2016) (by Francois Chollet)</li>	  
	    <li><b>Keras.IO: </b><a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" target="_blank">CNN using pre-trained GLoVe word embeddings</a> (2020) (by Francois Chollet)</li>
	    <li><b>Tensorflow: </b><a href="https://www.tensorflow.org/text/guide/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk" target="_blank">Retrieve the trained word embeddings and save them to disk</a> (2022) (by Tensorflow)</li>		   
      </ol>
	  
    </td>
  </tr>

  
  <tr>
    <td>Week 6 <b>17/04/2023</b> </td>

    <td>Early Neural Network Language Models (LM)
	<br>
    <td>

      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter)</li>		
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Chapter 7 - Neural Networks and Neural Language Models</a> (textbook chapter)</li>		
        <li>Bengio (2003): <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank"> A Neural Probabilistic Language Model</a> (original paper)</li>
	<li>Sebastian Ruder (2018): <a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/" target="_blank">A Review of the Neural History of Natural Language Processing</a></li>	      
      </ol>
		
      Python examples:
      <ol>	  
	 <li><b>machinelearningmastery.com: </b><a href="https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/" target="_blank">TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras</a> (by Jason Brownlee)</li>	  
  	 <li><b>Keras.IO</b>: <a href="https://keras.io/examples/generative/lstm_character_level_text_generation/" target="blank">Character-level text generation with LSTM</a></li>		
      </ol>
	  
    </td>
  </tr>
 
  <tr>
    <td>Week 7 <b>24/04/2023</b> </td>

    <td>RNNs & LSTMs for NLP
	<br>
    <td>

      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2019):</b> [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf" target="_blank">RNN slides</a>] </li>						
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Sections 9.1-9.6 (LSTM)</font> </a> </li>		
        <li>Goodfellow (2016): <a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank">Sequence Modeling: Recurrent and Recursive Neural Nets. 
		<font color="red">Sections 10.1-10.3, 10.5, 10.7-10.12</font> </a> </li>		
      </ol>
	  

      Suggested Tutorials:
      <ol>
	<li><a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target="_blank">A friendly introduction to Recurrent Neural Networks</a> (tutorial by Serrano)</li>
	<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding LSTM Networks</a> (blog post overview by Colah)</li>
      </ol>
	
      Suggested Cheatsheets:
      <ol>
	<li><a href="http://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" target="_blank">Recurrent Neural Networks cheatsheet </a> (CS 230 - Deep Learning)</li>
      </ol>
		
      <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
      <ol>	  
	<li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb" target="_blank">Chapter 16: Modeling Sequential Data Using Recurrent Neural Networks (Part 1/2)</a> - RNN layers (SimpleRNN, LSTM, GRU) <b>[see project one]</b>:</li>	  
      </ol> 

      Python examples:
      <ol>	  
        <li><b>Tensorflow.org</b>: <a href="https://www.tensorflow.org/text/tutorials/text_classification_rnn/" target="blank">Text classification with an RNN</a> (BiLSTM Sentiment Analysis Tutorial)</li>
   	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" target="blank">Bidirectional LSTM on IMDB</a></li>		
      </ol>	
 	  
    </td>
  </tr>
	
		
  <tr class="warning">
    <td><b>01/05/2023</b></td>
    <td>
      No class. Public Holiday
    </td>
    <td>
    </td>
  </tr>
		
		
  <tr>
    <td>Week 8 <b>08/05/2023</b> </td>
    <td>CNNs for NLP <br>
    </td>
		
    <td>
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2019):</b> [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes08-CNN.pdf" target="_blank">CNN slides</a>] </li>				
	<li>Goodfellow et al (2016): <a href="http://www.deeplearningbook.org/contents/convnets.html" target="_blank">Chapter 9 - Convolutional Networks (CNN)</a> (<i>Deep Learning</i> book chapter)</li>
        <li>Kim (2014): <a href="https://arxiv.org/abs/1408.5882" target="_blank">Convolutional Neural Networks for Sentence Classification</a></li>
        <li>Hinton et al (2012): <a href="https://arxiv.org/abs/1207.0580" target="_blank">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li>Kalchbrenner et al (2014): <a href="https://arxiv.org/abs/1404.2188" target="_blank">A Convolutional Neural Network for Modelling Sentences</a></li>
	<li>Zhang and Wallace (2016): <a href="https://arxiv.org/abs/1510.03820" target="_blank">A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification</a></li>
      </ol>
	  
      Suggested Tutorials:
      <ol>
	<li><a href="https://www.youtube.com/watch?v=2-Ol7ZB0MmU" target="_blank">A friendly introduction to Convolutional Neural Networks</a> (tutorial by Serrano)</li>
      </ol>		
		
      Suggested Cheatsheets:
      <ol>
	<li><a href="http://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" target="_blank">Convolutional Neural Networks cheatsheet</a> (CS 230 - Deep Learning)</li>
      </ol>

		
     Python examples:
      <ol>	  
	<li><b>Very good summary</b>: <a href="https://realpython.com/python-keras-text-classification/" target="_blank">Practical Text Classification With Python and Keras</a> (by Nikolai Janakiev)</li>		
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_from_scratch/" target="_blank">Text classification from scratch (using CNN)</a> (2020) (by Francois Chollet)</li>
        <li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" target="_blank">CNN using pre-trained GLoVe word embeddings</a> (2020) (by Francois Chollet)</li>		
      </ol>	
    </td>
  </tr>

 <tr>
    <td>Week 9 <b>15/05/2023</b> </td>
    <td>Seq2Seq (also called encoder-decoder), Machine Translation<br>
    
  </td>
    <td>
	
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2023):</b> [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture06-fancy-rnn.pdf" target="_blank">NMT slides</a>] </li>		
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Section 9.7 (Encoder-Decoder)</font></li>		
        <li>Sutskever et al. (2014): <a href="https://arxiv.org/abs/1409.3215" target="_blank">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</li>
	<li>Bahdanau et al. (2015): <a href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
	<li><a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank">BLUE</a> (original paper)</li>		
      </ol>

      Introduction Video:
      <ol>
    	<li>LT3 Research (2023): <a href="https://www.youtube.com/watch?v=NZTVm_0UTpc" target="_blank">What's inside a neural machine translation system?</a> (tutorial by LT3 Research)</li>		
     </ol>
		
      Suggested Tutorials:
      <ol>
        <li>Sutskever et al: <a href="https://paperswithcode.com/method/seq2seq" target="_blank">seq2seq explained</a> (tutorial by the authors)</li>		
      </ol>
		
      Python examples:
      <ol>	  
    	<li><b>Keras.IO</b>: <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank">A ten-minute introduction to sequence-to-sequence learning in Keras</a> (2017)</li>		
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/lstm_seq2seq/" target="blank">Character-level recurrent sequence-to-sequence model</a> (2020)</li>
      </ol>	
    </td>
 </tr>
		
 <tr>
    <td>Week 10 <b>22/05/2023</b> </td>
    <td>Self-Attention and Transformers<br>
  </td>
    <td>
	
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2023):</b> [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf" target="_blank">Transformer slides</a>] </li>				
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Sections 9.8-9.9 (Attention)</font></li>			
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf" target="_blank">Chapter 10 - Transformers and Pretrained Language Models</a> (textbook chapter). </li>	
	<li>Vasvani et al. (2017): <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a> (seminal paper)</li>
      </ol>	

      Suggested Tutorials:
      <ol>
        <li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">The Illustrated Attention</a> (tutorial by Jay Alammar)</li>
        <li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a> (tutorial by Jay Alammar)</li>	     
	<li><a href="https://machinelearningmastery.com/the-transformer-model/" target="_blank">Discovering the network architecture of the Transformer Model</a> (tutorial by Stefania Cristina)</li>
      </ol>	
	
      Suggested Blog Readings:
      <ol>
    	<li>Batista (2020): <a href="https://www.davidsbatista.net/blog/2020/01/25/Attention-seq2seq/" target="_blank">The Attention Mechanism in Natural Language Processing</a></li>				
        <li>Raschka (2023): <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html" target="_blank">Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</a></li>	     
      </ol>
		
      Python examples:
      <ol>	  
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_with_transformer/" target="_blank">Text classification with Transformer and Position Embeddings</a> (10 May 2020)</li>
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/neural_machine_translation_with_transformer/" target="_blank">English-to-Spanish translation with a sequence-to-sequence Transformer</a> (24 Feb 2022)</li>	
	<li><b>Tensorflow.org</b>: <a href="https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html" target="_blank">A Transformer Chatbot Tutorial with TensorFlow 2.0</a> (23 May 2019)</li> 
      </ol>	
    </td>
  </tr>

  
  <tr>
    <td>Week 11 <b>29/05/2023</b></td>
    <td>Pretrained Transformers
    <br>
    <td>
	
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2023)</b>: [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture9-pretraining.pdf" target="_blank">Pretraining slides</a>] </li>						
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf" target="_blank">Chapter 11 - Fine-Tuning and Masked Language Models</a> (textbook chapter)</li>
	<li><b>Raschka (2023)</b>: <a href="https://magazine.sebastianraschka.com/i/115060492/understanding-the-main-architecture-and-tasks" target="_blank">Understanding the Main Architecture and Tasks</a> - if you are new to transformers / large language models, it makes the most sense to start with this.</li> 
	<li>Devlin et al (2018): <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
        <li>Radford and Narasimhan (2018): <a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035" target="_blank">GPT: Improving Language Understanding by Generative Pre-Training</a></li>
	<li>Lewis et al (2019):	 <a href="https://arxiv.org/abs/1910.13461" target="_blank">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
        <li>Smith (2019): <a href="https://arxiv.org/abs/1902.06006" target="_blank">Contextual Word Representations: A Contextual Introduction</a></li>		
      </ol>
	  
      Suggested Tutorials:
      <ol>
	    <li><a href="http://jalammar.github.io/illustrated-bert/" target="_blank">The Illustrated BERT, ELMo, and co.</a> (tutorial by Jay Alammar)</li>
      	    <li><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" target="_blank">A Visual Guide to Using BERT for the First Time</a> (tutorial by Jay Alammar)</li>
	    <li><a href="http://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> (tutorial by Jay Alammar)</li>
            <li><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank">How GPT3 Works - Visualizations and Animations</a> (tutorial by Jay Alammar)</li>		
      </ol>

      Huggingface (very good reading):
      <ol>	  
	<li><a href="https://huggingface.co/learn/nlp-course/en/chapter1/4?fw=pt" target="_blank">How do Transformers work?</a></li>
	<li><a href="https://huggingface.co/learn/nlp-course/en/chapter1/5?fw=pt" target="_blank">Encoder models</a>: BERT-like (also called auto-encoding Transformer models)</li>		
	<li><a href="https://huggingface.co/learn/nlp-course/en/chapter1/6?fw=pt" target="_blank">Decoder models</a>: GPT-like (also called auto-regressive Transformer models)</li>				
	<li><a href="https://huggingface.co/learn/nlp-course/en/chapter1/6?fw=pt" target="_blank">Sequence-to-sequence models</a>: BART/T5-like (also called sequence-to-sequence Transformer models)</li>						
      </ol>
		
      Python examples:
      <ol>	  
	  <li><b>Hugginface.co</b>: <a href="https://huggingface.co/docs/transformers/tasks/sequence_classification" target="_blank"> Text classification</a>
	    <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/sequence_classification.ipynb" target="_blank"> [Open in Google Colab]</a>
	  </li>		
	  <li><b>Hugginface.co</b>: <a href="https://huggingface.co/docs/transformers/model_doc/bert" target="_blank"> BERT Text Classification</a>		
            <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb" target="_blank"> [Open in Google Colab]</a>
  	  </li>
      </ol>	

    </td>
  </tr>
  

  <tr>
    <td>Week 12 <b>05/06/2023</b></td>
    <td>
      Large Language Models (LLM)
      <br>
    </td>
    <td>
		
    Suggested readings:
      <ol>
	<li>Yang et al (2023): <a href="https://arxiv.org/abs/2304.13712" target="_blank">Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></li>		
	<li>Zhao et al (2023): <a href="https://arxiv.org/abs/2303.18223" target="_blank">A Survey of Large Language Models</a></li>		
	<li>Wei et al (2021): <a href="https://arxiv.org/abs/2109.01652" target="_blank">Finetuned Language Models Are Zero-Shot Learners</a></li>		
	<li>Brown et al (2020): <a href="https://openai.com/research/language-models-are-few-shot-learners" target="_blank">Language Models are Few-Shot Learners</a></li>
      </ol>
		
    Suggested tutorials:
      <ol>
	 <li>Raschka (2023): <a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models" target="_blank">Understanding Large Language Models -- A Cross-Section of the Most Relevant Literature To Get Up to Speed</a> (2023/04/16 tutorial)</li>
	 <li>Chaudhary (2020): <a href="https://amitness.com/2020/05/zero-shot-text-classification/" target="_blank">Zero Shot Learning for Text Classification</a></li>
	 <li>Davison (2020): <a href="https://joeddav.github.io/blog/2020/05/29/ZSL.html" target="_blank">Zero-Shot Learning in Modern NLP</a></li>
      </ol>		

		
    Python examples:
      <ol>	  
	  <li><b>OpenAI.com</b>: <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb" target="_blank">Zero-shot classification with embeddings</a></li>		
	  <li><b>Huggingface.co</b>: <a href="https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api" target="_blank">Few-shot learning in practice: GPT-Neo and the ðŸ¤— Accelerated Inference API</a></li>
      </ol>
		
    </td>		
  </tr>
		
  <tr>
    <td>Week 13 <b>12/06/2023</b></td>
    <td>
       No session.
    <br>
    </td>
    <td>
	Students works on their proposals.
    </td>
  </tr>
  
  <tr>
    <td>Week 14 <b>19/06/2023</b></td>
    <td>
      Student Presentations
      <br>
    </td>
    <td>		
		
      Topics:
      <ol>
        <li>Student #1: <a href="" target="_blank">Topic #1</a> </li>
        <li>Student #2: <a href="" target="_blank">Topic #2</a> </li>
      </ol>
    </td>
  </tr>
 
  <tr class="warning">
    <td>-</td>
    <td>
      NB! Practical Tips for NLP Projects
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/readings/final-project-practical-tips.pdf" target="_blank">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html" target="_blank">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
  </tr>

  <tr>
    <td></td>
    <td></td>
    <td>
		
  	(C) 2023 - Eduan KotzÃ© (PhD)

    </td>
  </tr>
		
  </tbody>
</table>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
