<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
  <title>CSIA7915</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>

<!-- Header -->
<div id="header" style="text-align:center">
  <h1>Natural Language Processing (CSIA7915) - 2023</h1><br>
  <h4>Adopted from <a href="http://web.stanford.edu/class/cs124/" target="_blank">CS124</a> and
  <a href="http://web.stanford.edu/class/cs224n/" target="_blank">CS224n</a> (Stanford University),
  <a href="http://markyatskar.com/cis530_sp2021/lectures.html" target="_blank">CIS530 Computational Linguistics</a>, and  
  <a href="https://www.uantwerpen.be/en/study/programmes/all-programmes/digital-text-analysis/" target="_blank">Digital Text Analytics</a> (University of Antwerp)</h4>    
  <b>Updated: 2023/02/14</b>
  <div style="clear:both;"></div>
</div>

<!-- Content -->
<div class="container sec" id="content">

  <h2>Prerequisites</h2>
  <ul>
      <li><b>Proficiency in Python</b>
          <p>Become familiar with Python 3 (using Numpy, pandas, NLTK, scikit-learn and Keras).</p>
          <p><u>The following resources are recommended if you new to Python Programming:</u></p>
          <ul><p>
              <li>Kaggle introduction to Python (very good): <a href="https://www.kaggle.com/learn/python" target="_blank">Kaggle - Python</a> - Good introduction if you are not familiar with programming.</li>		
	      <li>Google introduction to Python (excellent): <a href="https://developers.google.com/edu/python" target="_blank">Google’s Python Class</a> - If you know how to program but are not proficient in Python, I would recommend going through <b>Google’s Python Class</b> for people who already know how to code, taught by Nick Parlante.</li>	
          </p></ul>
          
          <p><u>Useful Python Resources:</u></p>
          <ul><p>
              <li>Jason Browlee. Machine Learning Mastery with Python: <a href="https://machinelearningmastery.com/" target="_blank">https://machinelearningmastery.com/</a></li>
              <li>TowardsDataScience: Machine Learning and NLP: <a href="https://towardsdatascience.com/machine-learning/home" target="_blank">https://towardsdatascience.com/machine-learning/home</a></li>
              <li>NLTK: <a href="https://www.nltk.org/" target="_blank">https://www.nltk.org/</a> - Natural Langauge Toolkit (Bird,2009)</li>
			  <li>Gensim: <a href="https://pypi.org/project/gensim/" target="_blank">https://pypi.org/project/gensim/</a> - Gensim, a library for topic modelling, document indexing and similarity retrieval with large corpora.</li>
			  <li>scikit-learn: <a href="https://scikit-learn.org/stable/" target="_blank">https://scikit-learn.org/stable/</a>
			  - The Scikit-Learn website has an impressive breadth of documentation and examples covering some of the models discussed here, and much, much more. 
			  If you want a brief survey of the most important and often-used machine learning algorithms, this website is a good place to start.</li>
          </p></ul>
		  
          <p><u>Useful Keras (Python) Resources:</u></p>
          <ul><p>
              <li>Keras blog: <a href="https://blog.keras.io" target="_blank">https://blog.keras.io</a> (very useful examples using Keras REST API)</li>	
              <li>Keras Home Page: <a href="https://keras.io/examples/nlp/" target="_blank">https://keras.io/examples/nlp/</a> (NLP examples by Chollet, creator of tf.Keras)</li>				  
          </p></ul>		  
      </li>
      
      <li><b>Calculus, Linear Algebra</b>
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b>
          <p>You should know basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
      <li><b>Foundations of Machine Learning</b>
          <p>You should know the basics of machine learning.
             There are many introductions to ML, in webpage, book, and video form. One approachable introduction is Hal Daum&eacute;’s in-progress <a href="http://ciml.info" target="_blank"><i>A Course in Machine Learning</i></a>. Reading the first 5 chapters of that book would be good background. Knowing the first 7 chapters would be even better!</p>
      </li>
  </ul>

  <h2>Reference Texts</h2>
  <p>
    The following texts are useful, and all of them can be read free online:
  </p>
  <ul>
      <li>Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Language Processing (3rd ed. draft)</a> [free downloads]</li>
      <li>Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank">Natural Language Processing</a> [free download]</li>
      <li>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/" target="_blank">Deep Learning</a> [free downloads]</li>
  </ul>
    <p>
    If you have no background in machine learning, the following book is <b>required reading</b>:
    </p>
    <ul>
      <li>
      Sebastian Raschka and Vahid Mirjalili. <a href="https://sebastianraschka.com/books/" target="_blank">Python Machine Learning. 3rd edition. 2019. Packt</a>
      </li>
  </ul>
    <p>
    If you have no background in neural networks, you might find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
      Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com" target="_blank">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank">Introduction to Deep Learning</a>
      </li>
  </ul>  
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Online Courses</h2>
<p>
The following courses I found useful in learning about machine learning and deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>#</th>
    <th>Description</th>
    <th>Online Course</th>
  </tr>
  </thead>
  <tbody>
   
  <tr>
    <td>1</td>
    <td>Introduction to General Machine Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Coursera: <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank">Machine Learning by Prof Andrew Ng (Stanford University)</a>  
		- This is a very clearly-taught free online course which covers the basics of machine learning from an algorithmic perspective. 
		It assumes undergraduate-level understanding of mathematics and programming, and steps through detailed considerations of some of the most important machine learning algorithms. 
		Homework assignments, which are algorithmically graded, have you actually implement some of these models yourself.</li>
      </ol>
      Suggested Online Tutorials:
      <ol>
        <li><b>Kaggle.com</b>: <a href="https://www.kaggle.com/learn/intro-to-machine-learning" target="_blank">Intro to Machine Learning</a> (covers how models work, model validation, underfitting, overfitting, random forest)</li>
        <li><b>Kaggle.com</b>: <a href="https://www.kaggle.com/learn/intermediate-machine-learning" target="_blank">Intermediate Machine Learning</a> (covers missing values, categorical variables, pipelines, cross-validation, XGBoost, data leakage)</li>
      </ol>
    </td>	
  </tr>

  <tr>
    <td>2</td>
    <td>Introduction to NLP (with Python), by Derek Jedamski
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>NLP with Python for Machine Learning Essential Training, LinkedIn Training: <a href="https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/welcome?u=37069596" target="_blank">https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training</a> [UFS login required]</li>
      </ol>
    </td>
  </tr>
	  

  <tr>
    <td>3</td>
    <td>Advanced NLP (with Python), by Derek Jedamski
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Advanced NLP with Python for Machine Learning, LinkedIn Training: <a href="https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning/" target="_blank">https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning/</a> [UFS login required]</li>
      </ol>
    </td>
  </tr>	  
  
 </tbody>
</table>
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Content</h2>
<p>
The learning material I consider important theoretical background if you want to become involve in NLP with machine learning and/or deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>Week</th>
    <th>Description</th>
    <th>Learning Materials</th>
  </tr>
  </thead>
  
  <tr>
    <td>1</td>
    <td>Introduction to the module
    </td>
    </td>
    <td>
      Rehash numpy, pandas, matplotlib. <br>    
      Suggested Reading: Jake VanderPlas, Python Data Science Handbook. Essential Tools for Working with Data. O'Reilly Media (2016):
      <ol>
        <li>numpy: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html" target="_blank">Introduction to NumPy</a></li>
        <li>pandas: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html" target="_blank">Data Manipulation with Pandas</a></li>
	<li>matplotlib: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html" target="_blank">Visualization with Matplotlib</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>2</td>
    <td>Text Preprocessing and Language Modelling
    <td>
  
	  Suggested Reading:
      <ol>
        <li>Wikipedia (n.d.): <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">Brief history of Natural Language Processing</a></li>
      </ol>
	  Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href=" https://web.stanford.edu/~jurafsky/slp3/2.pdf" target="_blank">Chapter 2 - Regular Expressions, Text Normalization, and Edit Distance</a> (textbook chapter)</li>
      </ol>
	  
      Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter). 
		<font color="red">Pages 1-16 (plus section 3.7 "Huge Language Models and Stupid Backoff")</font></li>
      </ol>
	    
	  Python examples:
      <ol>
	    <li>Gensim: <a href="https://radimrehurek.com/gensim/parsing/preprocessing.html" target="_blank">Preprocessing raw text</a></li>
	    <li>NLTK: <a href="https://www.kaggle.com/awadhi123/text-preprocessing-using-nltk/" target="_blank">Text preprocessing using NLTK in Python</a></li>
	    <li>Feature Engineering: <a href="https://scikit-learn.org/stable/modules/feature_extraction.html" target="blank">Text Feature Extraction (BoW, TFIDF, n-grams)</a></li>
      </ol>	 
	  
    </td>
  </tr>


  <tr>
    <td>3</td>
    <td>Text Classification and Sentiment Analysis
	<br>
		  NB [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf" target="_blank">slides</a>]<br>
		  Sentiment [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_Sent.pdf" target="_blank">slides</a>]
    </td>
    <td>
      Suggested Reading - Naive Bayes and Text Classification:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank">Chapter 4 - Naive Bayes Classification and Sentiment Classification</a> (textbook chapter). 
		<font color="red">pages 1-14 plus page 18, sections 4.1 through 4.8 and 4.10</font></li>
	<li>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. (2002): <a href=http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf" target="_blank">Thumbs up? Sentiment Classification using Machine Learning Techniques.</a> EMNLP 2002, pages 79-86</li>	      
      </ol>
	  
      Suggested Reading - Logistic Regression:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf" target="_blank">Chapter 5 - Logistic Regression</a> (textbook chapter). 
		<font color="red">pages 1-14. Pages 18-19 may also be useful! </font></li>		
      </ol>	 
	  
      Pipelines in sklearn:
      <ol>
        <li>Pipeline in sklearn: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html" target="_blank">Pipeline</a> </li>
	<li>Pipeline example in class.</li> 	
      </ol>
		
      <u>Python Example: Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	<ol>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb" target="_blank">Chapter 8 - Applying Machine Learning To Sentiment Analysis</a></li>
	</ol>
	  
     </td>
  </tr>
 
  <tr>
    <td>4</td>
    <td>Word Vectors
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture01-wordvecs1.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf" target="_blank">notes (lecture 1)</a>]
    </td>
    <td>
		
      Suggested Readings (word2vec):
      <ol>
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Chapter 6 - Vector Semantics and Embeddings</a> (textbook chapter). 
		<font color="red">pages 1-7, 17-26</font></li>
        <li>Mikolov et al (2013a): <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a> (original <b>word2vec</b> paper)</li>
        <li>Mikolov et al (2013b): <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
      </ol>		

      Suggested Readings (doc2vec and fasttext):
      <ol>
	<li>Le et al (2014): <a href="https://arxiv.org/abs/1405.4053" target="_blank">Distributed Representations of Sentences and Documents</a> (original <b>doc2vec</b> paper)</li>
        <li>Bonanowki et al (2017): <a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with Subword Information</a> (original <b>FastText</b> paper)</li>		
      </ol>
		
      Official Documentation:
      <ol>
        <li>Word2vec: <a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank">Word2vec embeddings</a> </li>
        <li>Doc2vec:  <a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank">Doc2vec paragraph embeddings</a> </li>
        <li>FastText: <a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank">FastText model</a> </li>			
      </ol>
		
      Suggested Tutorials:
      <ol>
        <li><a href="http://jalammar.github.io/illustrated-word2vec/" target="_blank">The Illustrated Word2Vec</a> (tutorial by Jay Alammar)</li>		
        <li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank">Word2Vec Tutorial - The Skip-Gram Model</a> (tutorial by McCormick)</li>
      </ol>	
		
      Gensim Learning-Oriented Lessons:
      <ol>	
	<li><b>Gensim</b> Word2Vec Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html" target="_blank">Word2Vec Model</a></li>
	<li><b>Gensim</b> Doc2Vec Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html" target="_blank">Doc2Vec Model</a></li>		
	<li><b>Gensim</b> FastText Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html" target="_blank">FastText Model</a></li>
      </ol>
    </td>
  </tr>
		
  <tr>
    <td>5</td>
    <td>Pre-Trained Word Vectors (glove)
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf" target="_blank">notes (lecture 2)</a>]<br>
	  [<a href="http://nlp.stanford.edu/projects/glove/" target="_blank">glove pretrained</a>]
      <br><br>
      Gensim word vectors example (GloVe):
      [<a href="http://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html" target="_blank">preview</a>]
    </td>
    <td>
		
    Suggested Readings:
      <ol>
        <li>Pennington et al (2014): <a href="http://aclweb.org/anthology/D/D14/D14-1162.pdf" target="_blank">GloVe: Global Vectors for Word Representation</a> (original <b>GloVe</b> paper)</li>
        <li>Levy et al (2015): <a href="http://www.aclweb.org/anthology/Q15-1016" target="_blank">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
	<li>Mikolov et al (2018): <a href="https://arxiv.org/abs/1712.09405" target="_blank">Advances in Pre-Training Distributed Word Representations</a></li>		
      </ol>

    Official Documentation:
    <ol>
        <li>Keras.io: <a href="https://keras.io/api/layers/core_layers/embedding/" target="_blank">Embedding layer.</a></li>
	<li>Gensim: <a href="https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow" target="_blank">Using Gensim Embeddings with Keras and Tensorflow.</a></li>	
    </ol>
	
    Python code of word embeddings:
      <ol>	  
	    <li><b>Keras.IO: </b><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank">Using pre-trained word embeddings in a Keras model</a> (2016) (by Francois Chollet)</li>	  
	    <li><b>Keras.IO: </b><a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" target="_blank">CNN using pre-trained GLoVe word embeddings</a> (2021) (by Francois Chollet)</li>
      </ol>
	  
    </td>
  </tr>

  
  <tr>
    <td>6</td>

    <td>Early Neural Network Langauge Models, Language Models (LM)
	<br>
		[<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
    <td>

	
      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter)</li>		
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Chapter 7 - Neural Networks and Neural Language Models</a> (textbook chapter)</li>		
        <li>Bengio (2003): <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank"> A Neural Probabilistic Language Model</a> (original paper)</li>
	<li>Sebastian Ruder (2018): <a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/" target="_blank">A Review of the Neural History of Natural Language Processing</a></li>	      
      </ol>
	  
    </td>
  </tr>
 
  <tr>
    <td>7</td>

    <td>RNNs & LSTMs
	<br>
		[<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
    <td>

      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Sections 9.1-9.6 (LSTM)</font></li>		
        <li>Goodfellow (2016): <a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank">Sequence Modeling: Recurrent and Recursive Neural Nets. 
		<font color="red">Sections 10.1-10.3, 10.5, 10.7-10.12</font></li>		
      </ol>
	  

      Suggested Tutorials:
      <ol>
	<li><a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target="_blank">A friendly introduction to Recurrent Neural Networks</a> (tutorial by Serrano)</li>
	<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding LSTM Networks</a> (blog post overview by Colah)</li>
      </ol>
			
      <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
      <ol>	  
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb" target="_blank">Chapter 16: Modeling Sequential Data Using Recurrent Neural Networks (Part 1/2)</a> - RNN layers (SimpleRNN, LSTM, GRU) <b>[see project one]</b>:</li>	  
      </ol> 

	  Python code of deep learning language models:
      <ol>	  
	    <li><b>machinelearningmastery.com: </b><a href="https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/" target="_blank">TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras</a> (by Jason Brownlee)</li>	  
            <li><b>Tensorflow.org</b>: <a href="https://www.tensorflow.org/text/tutorials/text_classification_rnn/" target="blank">Text classification with an RNN</a> (BiLSTM Sentiment Analysis Tutorial)</li>
   	    <li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" target="blank">Bidirectional LSTM on IMDB</a> (by Francois Chollet)</li>		
      </ol>	
      </ol> 
	  
    </td>
  </tr>
		
  <tr>
    <td>8</td>
    <td>CNNs for NLP <br>
	
	      [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
	      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes08-CNN.pdf" target="_blank">notes</a>]		  
	  
    </td>
    <td>
      Suggested Readings:
      <ol>
	<li>Goodfellow et al (2016): <a href="http://www.deeplearningbook.org/contents/convnets.html" target="_blank">Chapter 9 - Convolutional Networks (CNN)</a> (<i>Deep Learning</i> book chapter)</li>
        <li>Kim (2014): <a href="https://arxiv.org/abs/1408.5882.pdf" target="_blank">Convolutional Neural Networks for Sentence Classification</a></li>
        <li>Hinton et al (2012): <a href="https://arxiv.org/abs/1207.0580" target="_blank">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li>Kalchbrenner et al (2014): <a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank">A Convolutional Neural Network for Modelling Sentences</a></li>
      </ol>
	  
      Suggested Tutorials:
      <ol>
	<li><a href="https://www.youtube.com/watch?v=2-Ol7ZB0MmU" target="_blank">A friendly introduction to Convolutional Neural Networks</a> (tutorial by Serrano)</li>
      </ol>		

     Python code of deep learning language models:
      <ol>	  
		<li><b>Very good summary</b>: <a href="https://realpython.com/python-keras-text-classification/" target="blank">Practical Text Classification With Python and Keras</a> (by Nikolai Janakiev)</li>		
		<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_from_scratch/"  target="blank">Text classification from scratch using CNN</a> (by Francois Chollet)</li>
      </ol>	
    </td>
  </tr>

 <tr>
    <td>9</td>
    <td>Machine Translation, Seq2Seq (also called encoder-decoder)<br>
    [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
  </td>
    <td>
	
      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Sections 9.7 (Encoder-Decoder), 9.8-9.9 (Attention)</font></li>		
        <li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</li>
	<li><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
      </ol>

      Suggested Tutorials:
      <ol>
    	<li><a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a> (tutorial by Jay Alammar)</li>		
    	<li><a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank">A ten-minute introduction to sequence-to-sequence learning in Keras</a> (tutorial by Francois Chollet)</li>
      </ol>
		
      Python code:
      <ol>	  
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/lstm_seq2seq/" target="blank">Character-level recurrent sequence-to-sequence model</a> (by Francois Chollet)</li>
      </ol>	
    </td>
 </tr>
		
 <tr>
    <td>10</td>
    <td>Transformers, Attention<br>
    [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
  </td>
    <td>
	
      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf" target="_blank">Chapter 10 - Transformers and Pretrained Language Models</a> (textbook chapter). </li>		
      </ol>	

      Suggested Tutorials:
      <ol>
        <li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a> (tutorial by Jay Alammar)</li>	     
	<li><a href="https://machinelearningmastery.com/the-transformer-model/" target="_blank">Discovering the network architecture of the Transformer Model</a> (tutorial by Stefania Cristina)</li>
      </ol>	
		
      Suggested Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/1706.03762.pdf" target="_blank">Attention Is All You Need</a> (seminal paper)</li>
        <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">Transformer (Google AI blog post)</a></li>
        <li><a href="https://arxiv.org/pdf/1802.05751.pdf" target="_blank">Image Transformer</a></li>
      </ol>
	  
	  Python code:
      <ol>	  
		
	<li><b>machinelearningmastery.com</b>: <a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" target="blank">Introduction to Positional Encoding In Transformer Models, Part 1</a> (by Mehreen Saeed)</li>
	<li><b>machinelearningmastery.com</b>: <a href="https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/" target="blank">Transformer Positional Encoding Layer in Keras, Part 2</a>  (by Mehreen Saeed)</li>
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_with_transformer/" target="blank">Text classification with Transformer and Position Embeddings</a></li>
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/neural_machine_translation_with_transformer/" target="blank">English-to-Spanish translation with a sequence-to-sequence Transformer</a></li>	
      </ol>	
    </td>
  </tr>

 
  
  <tr>
    <td>11</td>
    <td>More about Transformers and Pretraining
    <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
    <td>
	
      Suggested readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf" target="_blank">Chapter 11 - Fine-Tuning and Masked Language Models</a> (textbook chapter)</li>
	<li>Devlin et al (2018) <a href="https://arxiv.org/abs/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
        <li>Smith (2019): <a href="https://arxiv.org/abs/1902.06006.pdf" target="blank">Contextual Word Representations: A Contextual Introduction</a></li>			

      </ol>
	  
	  Suggested readings (Tutorials):
      <ol>
	    <li><a href="http://www.davidsbatista.net//blog/2018/12/06/Word_Embeddings/" target="_blank">Language Models and Contextualised Word Embeddings</a> (tutorial by David S. Batista)</li>
	    <li><a href="http://jalammar.github.io/illustrated-bert/" target="_blank">The Illustrated BERT, ELMo, and co.</a> (tutorial by Jay Alammar)</li>
      	    <li><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" target="_blank">A Visual Guide to Using BERT for the First Time</a> (tutorial by Jay Alammar)</li>
	    <li><a href="http://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> (tutorial by Jay Alammar)</li>
            <li><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank">How GPT3 Works - Visualizations and Animations</a> (tutorial by Jay Alammar)</li>		
      </ol>
	  
	  Python code:
      <ol>	  
	  <li><b>Tensorflow.org</b>: <a href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert/" target="blank">Classify text with BERT</a> (BERT Sentiment Analysis Tutorial)</li>
	  <li><b>Hugginface.co</b>: <a href="https://huggingface.co/course/chapter1/1" target="blank">Using Huggingface Transformer Models</a></li>		
      </ol>	

    </td>
  </tr>
  

  <tr>
    <td>12</td>
    <td>
      Conversational Agents (aka "Chatbots")
      <br>
    </td>
    <td>
    Suggested readings:
      <ol>
		<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/24.pdf" target="_blank">Chapter 24 - Dialogue Systems and Chatbots</a> (textbook chapter)</li>
		<li>Liu et al (2016):    <a href="https://arxiv.org/abs/1603.08023.pdf" target="_blank">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a></li>
	        <li>Kottur et al (2017): <a href=" https://www.ijcai.org/proceedings/2017/0521.pdf" target="_blank">Exploring Personalized Neural Conversational Models</a></li>		
	        <li>Gao et al (2019):    <a href="http://arxiv.org/abs/1809.08267" target="_blank">Neural Approaches to Conversational AI</a></li>		
      </ol>
		
    Additional readings:
      <ol>
		<li>Haixun Wang (2018): <a href="https://medium.com/x8-the-ai-community/a-reading-list-and-mini-survey-of-conversational-ai-32fceea97180" target="_blank">An Annotated Reading List of Conversational AI</a> (tutorial)</li>
      </ol>		
    </td>
  </tr>
  

  <tr>
    <td>13</td>
    <td>
      Topic Modeling
      <br>
    </td>
    <td>		
		
      Official Documentation:
      <ol>
        <li>Topic Modelling with LDA: <a href="https://radimrehurek.com/gensim/models/ldamodel.html" target="_blank">LDA Model</a> </li>
      </ol>
				
      Gensim Learning-Oriented Lessons:
      <ol>
	<li><b>Gensim LDA: </b><a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html" target="_blank">LDA Model</a></li>
      </ol>
    </td>
  </tr>
		
		
 
  <tr class="warning">
    <td>14</td>
    <td>
      NB! Practical Tips for NLP Projects
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/readings/final-project-practical-tips.pdf" target="_blank">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html" target="_blank">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
  </tr>

  
  </tbody>
</table>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
