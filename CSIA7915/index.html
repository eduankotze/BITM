<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
  <title>CSIA7915</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>

<!-- Header -->
<div id="header" style="text-align:center">
  <h1>Natural Language Processing (CSIA7915) - 2023</h1><br>
  <h4>Adopted from <a href="http://web.stanford.edu/class/cs124/" target="_blank">CS124</a> and
  <a href="http://web.stanford.edu/class/cs224n/" target="_blank">CS224n</a> (Stanford University),
  <a href="https://www.cis.upenn.edu/~myatskar/teaching/cis530_fa22//lectures.html" target="_blank">CIS530 Computational Linguistics</a>, and  
  <a href="https://www.uantwerpen.be/en/study/programmes/all-programmes/digital-text-analysis/" target="_blank">Digital Text Analytics</a> (University of Antwerp)</h4>    
  <b>Updated: 2023/04/17</b>
  <div style="clear:both;"></div>
</div>

<!-- Content -->
<div class="container sec" id="content">

  <h2>Prerequisites</h2>
  <ul>
      <li><b>Proficiency in Python</b>
          <p>Become familiar with Python 3 (using Numpy, pandas, NLTK, scikit-learn and Keras).</p>
          <p><u>The following resources are recommended if you new to Python Programming:</u></p>
          <ul><p>
              <li>Kaggle introduction to Python (very good): <a href="https://www.kaggle.com/learn/python" target="_blank">Kaggle - Python</a> - Good introduction if you are not familiar with programming.</li>		
	      <li>Google introduction to Python (excellent): <a href="https://developers.google.com/edu/python" target="_blank">Google’s Python Class</a> - If you know how to program but are not proficient in Python, I would recommend going through <b>Google’s Python Class</b> for people who already know how to code, taught by Nick Parlante.</li>	
          </p></ul>
          
          <p><u>Useful Python Resources:</u></p>
          <ul><p>
              <li>Jason Browlee. Machine Learning Mastery with Python: <a href="https://machinelearningmastery.com/" target="_blank">https://machinelearningmastery.com/</a></li>
              <li>TowardsDataScience: Machine Learning and NLP: <a href="https://towardsdatascience.com/machine-learning/home" target="_blank">https://towardsdatascience.com/machine-learning/home</a></li>
              <li>NLTK: <a href="https://www.nltk.org/" target="_blank">https://www.nltk.org/</a> - Natural Langauge Toolkit (Bird, 2009)</li>
	      <li>Gensim: <a href="https://pypi.org/project/gensim/" target="_blank">https://pypi.org/project/gensim/</a> - Gensim, a library for topic modelling, document indexing and similarity retrieval with large corpora.</li>
	      <li>scikit-learn: <a href="https://scikit-learn.org/stable/" target="_blank">https://scikit-learn.org/stable/</a>
		  - The Scikit-Learn website has an impressive breadth of documentation and examples covering some of the models discussed here, and much, much more. 
		  If you want a brief survey of the most important and often-used machine learning algorithms, this website is a good place to start.</li>
          </p></ul>
		  
          <p><u>Useful Keras (Python) Resources:</u></p>
          <ul><p>
              <li>Keras blog: <a href="https://blog.keras.io" target="_blank">https://blog.keras.io</a> (very useful examples using Keras REST API)</li>	
              <li>Keras Home Page: <a href="https://keras.io/examples/nlp/" target="_blank">https://keras.io/examples/nlp/</a> (NLP examples by Chollet, creator of tf.Keras)</li>				  
          </p></ul>		  
      </li>
      
      <li><b>Calculus, Linear Algebra</b>
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b>
          <p>You should know basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
      <li><b>Foundations of Machine Learning</b>
          <p>You should know the basics of machine learning.
             There are many introductions to ML, in webpage, book, and video form. One approachable introduction is Hal Daum&eacute;’s in-progress <a href="http://ciml.info" target="_blank"><i>A Course in Machine Learning</i></a>. Reading the first 5 chapters of that book would be good background. Knowing the first 7 chapters would be even better!</p>
      </li>
  </ul>

  <h2>Reference Texts</h2>
  <p>
    The following texts are useful for theoretical background, and all of them can be read free online:
  </p>
  <ul>
      <li>Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Language Processing (3rd ed. draft)</a> [free downloads]</li>
      <li>Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank">Natural Language Processing</a> [free download]</li>
      <li>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/" target="_blank">Deep Learning</a> [free downloads]</li>
  </ul>
    <p>
    The following hands-on books are useful for NLP, machine and deep learning:
    </p>
    <ul>
      <li>Tunstall, Von Werra and Wolf. <a href="https://transformersbook.com/" target="_blank">Natural Language Processing with Transformers. 1st edition. 2022. O'Reilly</a></li>
      <li>Sebastian Raschka and Vahid Mirjalili. <a href="https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn" target="_blank">Python Machine Learning. 4th edition. 2022. Packt</a></li>
      <li>Raaijmakers. <a href="https://www.manning.com/books/deep-learning-for-natural-language-processing" target="_blank">Deep Learning for Natural Language Processing. 1st edition. 2022.</a></li>
      <li>Cholet. <a href="https://www.manning.com/books/deep-learning-with-python-second-edition" target="_blank">Deep Learning with Python. 2nd edition. 2021</a></li>
      <li>Azunre. <a href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" target="_blank">Transfer Learning for Natural Language Processing. 1st edition. 2021</a></li>
      <li>Lane, Howard and Hapke. <a href="https://www.manning.com/books/natural-language-processing-in-action" target="_blank">Natural Language Processing in Action. 1st edition. 2019</a></li>	    
  </ul>
    <p>
    If you have no background in neural networks, you might find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
      Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com" target="_blank">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank">Introduction to Deep Learning</a>
      </li>
  </ul>  


  <h2>Online Material</h2>
  <p>
    The following online resources will help you stay informed with the field of NLP:
  </p>
  <ul>
      <li><a href="https://paperswithcode.com/methods/area/natural-language-processing" target="_blank">Papers with Code</a></li>
      <li><a href="https://madewithml.com/topics/natural-language-processing/" target="_blank">Made with ML</a></li>
      <li><a href="https://nlpprogress.com/" target="_blank">NLP Progress</a></li>	  
      <li><a href="https://ruder.io/nlp-news/" target="_blank">NLP News by Sebastian Ruder</a></li>		  
  </ul>
</div>
	
<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Online Courses</h2>
<p>
The following courses I found useful in learning about machine learning and deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>#</th>
    <th>Description</th>
    <th>Online Course</th>
  </tr>
  </thead>
  <tbody>
   
  <tr>
    <td>1</td>
    <td>Introduction to General Machine Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Coursera: <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank">Machine Learning by Prof Andrew Ng (Stanford University)</a>  
		- This is a very clearly-taught free online course which covers the basics of machine learning from an algorithmic perspective. 
		It assumes undergraduate-level understanding of mathematics and programming, and steps through detailed considerations of some of the most important machine learning algorithms. 
		Homework assignments, which are algorithmically graded, have you actually implement some of these models yourself.</li>
      </ol>
      Suggested Online Tutorials:
      <ol>
        <li><b>Kaggle.com</b>: <a href="https://www.kaggle.com/learn/intro-to-machine-learning" target="_blank">Intro to Machine Learning</a> (covers how models work, model validation, underfitting, overfitting, random forest)</li>
        <li><b>Kaggle.com</b>: <a href="https://www.kaggle.com/learn/intermediate-machine-learning" target="_blank">Intermediate Machine Learning</a> (covers missing values, categorical variables, pipelines, cross-validation, XGBoost, data leakage)</li>
      </ol>
    </td>	
  </tr>

  <tr>
    <td>2</td>
    <td>Introduction to NLP (with Python), by Derek Jedamski
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>NLP with Python for Machine Learning Essential Training, LinkedIn Training: <a href="https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/welcome?u=37069596" target="_blank">https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training</a> [UFS login required]</li>
      </ol>
    </td>
  </tr>
	  

  <tr>
    <td>3</td>
    <td>Advanced NLP (with Python), by Derek Jedamski
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Advanced NLP with Python for Machine Learning, LinkedIn Training: <a href="https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning/" target="_blank">https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning/</a> [UFS login required]</li>
      </ol>
    </td>
  </tr>	  
  
 </tbody>
</table>
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Content</h2>
<p>
The learning material I consider important theoretical background if you want to become involve in NLP with machine learning and/or deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>Week</th>
    <th>Description</th>
    <th>Learning Materials</th>
  </tr>
  </thead>
  
  <tr>
    <td>Week 1 <b>27/02/2023</b> </td> 
    <td>Introduction to the module
    </td>
    </td>
    <td>
      Rehash numpy, pandas, matplotlib. <br>    
      Suggested Reading: Jake VanderPlas, Python Data Science Handbook. Essential Tools for Working with Data. O'Reilly Media (2016):
      <ol>
        <li>numpy: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html" target="_blank">Introduction to NumPy</a></li>
        <li>pandas: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html" target="_blank">Data Manipulation with Pandas</a></li>
	<li>matplotlib: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html" target="_blank">Visualization with Matplotlib</a></li>
      </ol>
     	    
      Machine Learning Fundamentals (with scikit-learn):
      <ol>
      <li><a href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank">Cross-validation: evaluating estimator performance (3.1)</a></li>
      <li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html" target="_blank">Visualizing cross-validation</a></li>	      
      <li><a href="https://scikit-learn.org/stable/modules/grid_search.html" target="_blank">Tuning the hyper-parameters of an estimator (3.2)</a></li>
      <li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html" target="_blank">Metrics and scoring: quantifying the quality of predictions (3.3)</a></li>	      
      <li><a href="https://scikit-learn.org/stable/modules/learning_curve.html#validation-curve" target="_blank">Validation curve (3.4)</a></li>	      
      <li><a href="https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve" target="_blank">Learning curve (3.4)</a></li>	      
      <li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html" target="_blank">Learning Curve (example)</a></li>	      
      </ol>
    </td>
  </tr>

  <tr>
    <td>Week 2 <b>06/03/2023</b> </td>
    <td>Text Preprocessing and Language Modelling
    <td>
	  Suggested Reading:
      <ol>
        <li>Wikipedia (n.d.): <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">Brief history of Natural Language Processing</a></li>
      </ol>
	  Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href=" https://web.stanford.edu/~jurafsky/slp3/2.pdf" target="_blank">Chapter 2 - Regular Expressions, Text Normalization, and Edit Distance</a> (textbook chapter)</li>
      </ol>
	  
      Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter). 
		<font color="red">Pages 1-16 (plus section 3.7 "Huge Language Models and Stupid Backoff")</font></li>
      </ol>
	    
	  Python examples:
      <ol>
	    <li>Gensim: <a href="https://radimrehurek.com/gensim/parsing/preprocessing.html" target="_blank">Preprocessing raw text</a></li>
	    <li>NLTK: <a href="https://www.kaggle.com/awadhi123/text-preprocessing-using-nltk/" target="_blank">Text preprocessing using NLTK in Python</a></li>
	    <li>scikit-learn (feature extraction): <a href="https://scikit-learn.org/stable/modules/feature_extraction.html" target="blank">Text Feature Extraction (BoW, TFIDF, n-grams)</a></li>
      </ol>	 
	  
    </td>
  </tr>


  <tr>
    <td>Week 3 <b>13/03/2023</b> </td>
    <td>Text Classification and Sentiment Analysis
	<br>
		  NB [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf" target="_blank">slides</a>] <br>
		  Sentiment [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_Sent.pdf" target="_blank">slides</a>] <br>
	          Lexicons [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/20_Affect_Jan_18_20201.pdf" target="_blank">slides</a>]
    </td>
    <td>
      Suggested Reading - Naive Bayes and Text Classification:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank">Chapter 4 - Naive Bayes Classification and Sentiment Classification</a> (textbook chapter). 
		<font color="red">pages 1-14 plus page 18, sections 4.1 through 4.8 and 4.10</font></li>
	<li>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. (2002): <a href=http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf" target="_blank">Thumbs up? Sentiment Classification using Machine Learning Techniques.</a> EMNLP 2002, pages 79-86</li>	      
      </ol>
	  
      Suggested Reading - Logistic Regression:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf" target="_blank">Chapter 5 - Logistic Regression</a> (textbook chapter). 
		<font color="red">pages 1-14. Pages 18-19 may also be useful! </font></li>		
      </ol>	 

      Suggested Reading - Lexicons:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/25.pdf" target="_blank">Chapter 25 - Lexicons for Sentiment, Affect, and Connotation</a> (textbook chapter).</li>		
      </ol>	
		
      Pipelines in sklearn:
      <ol>
        <li><a href="https://scikit-learn.org/stable/modules/compose.html" target="_blank">scikit-learn (pipeline)</a></li>
	<li><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_text_feature_extraction.html">Sample pipeline for text feature extraction and evaluation</a></li>
	<li><a href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">Working With Text Data (features, pipeline, grid search)</a></li>		
      </ol>
		
      Ensembles in sklearn:
      <ol>
        <li><a href="https://scikit-learn.org/stable/modules/ensemble.html" target="_blank">Ensemble methods</a></li>
	<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" target="_blank">VotingClassifier</a></li>
	<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html" target="_blank">StackingClassifier</a></li>		
      </ol>
		
      <u>Python Example: Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	<ol>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb" target="_blank">Chapter 8 - Applying Machine Learning To Sentiment Analysis</a></li>
	</ol>
	  
     </td>
  </tr>
 
  <tr>
    <td>Week 4 <b>27/03/2023</b> </td>
    <td>Word Vectors
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture01-wordvecs1.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf" target="_blank">notes (lecture 1)</a>]
    </td>
    <td>
		
      Suggested Readings (word2vec):
      <ol>
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Chapter 6 - Vector Semantics and Embeddings</a> (textbook chapter). 
		<font color="red">pages 1-7, 17-26</font></li>
        <li>Mikolov et al (2013a): <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a> (original <b>word2vec</b> paper)</li>
        <li>Mikolov et al (2013b): <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
      </ol>		

      Suggested Readings (doc2vec and fasttext):
      <ol>
	<li>Le et al (2014): <a href="https://arxiv.org/abs/1405.4053" target="_blank">Distributed Representations of Sentences and Documents</a> (original <b>doc2vec</b> paper)</li>
        <li>Bonanowki et al (2017): <a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with Subword Information</a> (original <b>FastText</b> paper)</li>		
      </ol>
		
      Official Documentation:
      <ol>
        <li>Word2vec: <a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank">Word2vec embeddings</a> </li>
        <li>Doc2vec:  <a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank">Doc2vec paragraph embeddings</a> </li>
        <li>FastText: <a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank">FastText model</a> </li>			
      </ol>
		
      Suggested Tutorials:
      <ol>
        <li><a href="http://jalammar.github.io/illustrated-word2vec/" target="_blank">The Illustrated Word2Vec</a> (tutorial by Jay Alammar)</li>		
        <li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank">Word2Vec Tutorial - The Skip-Gram Model</a> (tutorial by McCormick)</li>
      </ol>	
		
      Gensim Learning-Oriented Lessons:
      <ol>	
	<li><b>Gensim</b> Word2Vec Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html" target="_blank">Word2Vec Model</a></li>
	<li><b>Gensim</b> Doc2Vec Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html" target="_blank">Doc2Vec Model</a></li>		
	<li><b>Gensim</b> FastText Model: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html" target="_blank">FastText Model</a></li>
      </ol>
    </td>
  </tr>
		
  <tr>
    <td>Week 5 <b>03/04/2023</b> </td>
    <td>Pre-Trained Word Vectors (glove)
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf" target="_blank">notes (lecture 2)</a>]<br>
	  [<a href="http://nlp.stanford.edu/projects/glove/" target="_blank">glove pretrained</a>]
      <br><br>
      Gensim word vectors example (GloVe):
      [<a href="http://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html" target="_blank">preview</a>]
    </td>
    <td>
		
    Suggested Readings:
      <ol>
        <li>Pennington et al (2014): <a href="http://aclweb.org/anthology/D/D14/D14-1162.pdf" target="_blank">GloVe: Global Vectors for Word Representation</a> (original <b>GloVe</b> paper)</li>
        <li>Levy et al (2015): <a href="http://www.aclweb.org/anthology/Q15-1016" target="_blank">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
	<li>Mikolov et al (2018): <a href="https://arxiv.org/abs/1712.09405" target="_blank">Advances in Pre-Training Distributed Word Representations</a></li>		
      </ol>

    Official Documentation:
    <ol>
        <li>Keras.io: <a href="https://keras.io/api/layers/core_layers/embedding/" target="_blank">Embedding layer.</a></li>
	<li>Gensim: <a href="https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow" target="_blank">Using Gensim Embeddings with Keras and Tensorflow.</a></li>	
    </ol>
	
    Python code of word embeddings:
      <ol>	  
	    <li><b>Keras.IO: </b><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank">Using pre-trained word embeddings in a Keras model</a> (2016) (by Francois Chollet)</li>	  
	    <li><b>Keras.IO: </b><a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" target="_blank">CNN using pre-trained GLoVe word embeddings</a> (2021) (by Francois Chollet)</li>
	    <li><b>Tensorflow: </b><a href="https://www.tensorflow.org/text/guide/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk" target="_blank">Retrieve the trained word embeddings and save them to disk</a> (2022) (by Tensorflow)</li>		   
      </ol>
	  
    </td>
  </tr>

  
  <tr>
    <td>Week 6 <b>17/04/2023</b> </td>

    <td>Early Neural Network Language Models (LM)
	<br>
    <td>

      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter)</li>		
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Chapter 7 - Neural Networks and Neural Language Models</a> (textbook chapter)</li>		
        <li>Bengio (2003): <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank"> A Neural Probabilistic Language Model</a> (original paper)</li>
	<li>Sebastian Ruder (2018): <a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/" target="_blank">A Review of the Neural History of Natural Language Processing</a></li>	      
      </ol>
		
      Python code:
      <ol>	  
	 <li><b>machinelearningmastery.com: </b><a href="https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/" target="_blank">TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras</a> (by Jason Brownlee)</li>	  
  	 <li><b>Keras.IO</b>: <a href="https://keras.io/examples/generative/lstm_character_level_text_generation/" target="blank">Character-level text generation with LSTM</a></li>		
      </ol>
	  
    </td>
  </tr>
 
  <tr>
    <td>Week 7 <b>24/04/2023</b> </td>

    <td>RNNs & LSTMs for NLP
	<br>
    <td>

      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2019):</b> [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf" target="_blank">RNN slides</a>] </li>						
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Sections 9.1-9.6 (LSTM)</font> </a> </li>		
        <li>Goodfellow (2016): <a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank">Sequence Modeling: Recurrent and Recursive Neural Nets. 
		<font color="red">Sections 10.1-10.3, 10.5, 10.7-10.12</font> </a> </li>		
      </ol>
	  

      Suggested Tutorials:
      <ol>
	<li><a href="https://www.youtube.com/watch?v=UNmqTiOnRfg" target="_blank">A friendly introduction to Recurrent Neural Networks</a> (tutorial by Serrano)</li>
	<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding LSTM Networks</a> (blog post overview by Colah)</li>
      </ol>
			
      <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
      <ol>	  
	<li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb" target="_blank">Chapter 16: Modeling Sequential Data Using Recurrent Neural Networks (Part 1/2)</a> - RNN layers (SimpleRNN, LSTM, GRU) <b>[see project one]</b>:</li>	  
      </ol> 

      Python code:
      <ol>	  
        <li><b>Tensorflow.org</b>: <a href="https://www.tensorflow.org/text/tutorials/text_classification_rnn/" target="blank">Text classification with an RNN</a> (BiLSTM Sentiment Analysis Tutorial)</li>
   	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" target="blank">Bidirectional LSTM on IMDB</a></li>		
      </ol>	
 	  
    </td>
  </tr>
		
  <tr>
    <td>Week 8 <b>08/05/2023</b> </td>
    <td>CNNs for NLP <br>
    </td>
		
    <td>
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2019):</b> [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes08-CNN.pdf" target="_blank">CNN slides</a>] </li>				
	<li>Goodfellow et al (2016): <a href="http://www.deeplearningbook.org/contents/convnets.html" target="_blank">Chapter 9 - Convolutional Networks (CNN)</a> (<i>Deep Learning</i> book chapter)</li>
        <li>Kim (2014): <a href="https://arxiv.org/abs/1408.5882.pdf" target="_blank">Convolutional Neural Networks for Sentence Classification</a></li>
        <li>Hinton et al (2012): <a href="https://arxiv.org/abs/1207.0580" target="_blank">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li>Kalchbrenner et al (2014): <a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank">A Convolutional Neural Network for Modelling Sentences</a></li>
      </ol>
	  
      Suggested Tutorials:
      <ol>
	<li><a href="https://www.youtube.com/watch?v=2-Ol7ZB0MmU" target="_blank">A friendly introduction to Convolutional Neural Networks</a> (tutorial by Serrano)</li>
      </ol>		

     Python code:
      <ol>	  
	<li><b>Very good summary</b>: <a href="https://realpython.com/python-keras-text-classification/" target="blank">Practical Text Classification With Python and Keras</a> (by Nikolai Janakiev)</li>		
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_from_scratch/"  target="blank">Text classification from scratch using CNN</a></li>
      </ol>	
    </td>
  </tr>

 <tr>
    <td>Week 9 <b>15/05/2023</b> </td>
    <td>Seq2Seq (also called encoder-decoder), Machine Translation<br>
    
  </td>
    <td>
	
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2023):</b> [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture06-fancy-rnn.pdf" target="_blank">slides</a>] </li>		
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Section 9.7 (Encoder-Decoder)</font></li>		
        <li>Sutskever et al. (2014): <a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank">Sequence to Sequence Learning with Neural Networks</a> (original seq2seq NMT paper)</li>
	<li>Bahdanau et al. (2015): <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a> (original seq2seq+attention paper)</li>
	<li><a href="https://www.aclweb.org/anthology/P02-1040.pdf" target="_blank">BLUE</a> (original paper)</li>		
      </ol>

      Introduction Video:
      <ol>
    	<li>LT3 Research (2023): <a href="https://www.youtube.com/watch?v=NZTVm_0UTpc" target="_blank">What's inside a neural machine translation system?</a> (tutorial by LT3 Research)</li>		
     </ol>
		
      Suggested Tutorials:
      <ol>
    	<li><a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a> (tutorial by Jay Alammar)</li>		

      </ol>
		
      Python code:
      <ol>	  
    	<li><b>Keras.IO</b>: <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" target="_blank">A ten-minute introduction to sequence-to-sequence learning in Keras</a> (2017)</li>		
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/lstm_seq2seq/" target="blank">Character-level recurrent sequence-to-sequence model</a> (2020)</li>
      </ol>	
    </td>
 </tr>
		
 <tr>
    <td>Week 10 <b>22/05/2023</b> </td>
    <td>Self-Attention and Transformers<br>
  </td>
    <td>
	
      Suggested Readings:
      <ol>
	<li><b>Stanford CS224N (2023):</b> [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture08-transformers.pdf" target="_blank">slides</a>] </li>				
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - RNNs and LSTMs</a> (textbook chapter). 
		<font color="red">Sections 9.8-9.9 (Attention)</font></li>			
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf" target="_blank">Chapter 10 - Transformers and Pretrained Language Models</a> (textbook chapter). </li>		
      </ol>	

      Suggested Tutorials:
      <ol>
        <li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a> (tutorial by Jay Alammar)</li>	     
	<li><a href="https://machinelearningmastery.com/the-transformer-model/" target="_blank">Discovering the network architecture of the Transformer Model</a> (tutorial by Stefania Cristina)</li>
      </ol>	
		
      Suggested Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/1706.03762.pdf" target="_blank">Attention Is All You Need</a> (seminal paper)</li>
        <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">Transformer (Google AI blog post)</a></li>
        <li><a href="https://arxiv.org/pdf/1802.05751.pdf" target="_blank">Image Transformer</a></li>
      </ol>
	  
      Python code:
      <ol>	  
	<li><b>Raschka</b>: <a href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html" target="_blank">Understanding Attention, Self-Attention, and Multi-Head Attention	</a></li>
	<li><b>machinelearningmastery.com</b>: <a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" target="_blank">Introduction to Positional Encoding In Transformer Models, Part 1</a> (by Mehreen Saeed)</li>
	<li><b>machinelearningmastery.com</b>: <a href="https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/" target="_blank">Transformer Positional Encoding Layer in Keras, Part 2</a>  (by Mehreen Saeed)</li>
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_with_transformer/" target="_blank">Text classification with Transformer and Position Embeddings</a></li>
	<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/neural_machine_translation_with_transformer/" target="_blank">English-to-Spanish translation with a sequence-to-sequence Transformer</a></li>	
      </ol>	
    </td>
  </tr>

 
  
  <tr>
    <td>Week 11 <b>29/05/2023</b></td>
    <td>Pretrained Transformers
    <br>
    <td>
	
      Suggested readings:
      <ol>
	<li><b>Stanford CS224N (2023)</b>: [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture9-pretraining.pdf" target="_blank">slides</a>] </li>						
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf" target="_blank">Chapter 11 - Fine-Tuning and Masked Language Models</a> (textbook chapter)</li>
	<li>Devlin et al (2018): <a href="https://arxiv.org/abs/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
        <li>Radford and Narasimhan (2018): <a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035" target="_blank">GPT: Improving Language Understanding by Generative Pre-Training</a></li>
        <li>Smith (2019): <a href="https://arxiv.org/abs/1902.06006.pdf" target="_blank">Contextual Word Representations: A Contextual Introduction</a></li>	
        <li>Xavier Amatriain (2023): <a href="https://github.com/eduankotze/NMT/blob/main/papers/Transformer%20models%20an%20introduction%20and%20catalog.pdf" target="_blank">Tranformer Models: An Introduction and Catalog.</li>				
      </ol>
	  
	  Suggested readings (Tutorials):
      <ol>
	    <li><a href="http://jalammar.github.io/illustrated-bert/" target="_blank">The Illustrated BERT, ELMo, and co.</a> (tutorial by Jay Alammar)</li>
      	    <li><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" target="_blank">A Visual Guide to Using BERT for the First Time</a> (tutorial by Jay Alammar)</li>
	    <li><a href="http://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> (tutorial by Jay Alammar)</li>
            <li><a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" target="_blank">How GPT3 Works - Visualizations and Animations</a> (tutorial by Jay Alammar)</li>		
      </ol>
	  
	  Python code:
      <ol>	  
	  <li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/pretraining_BERT/" target="_blank">Pretraining BERT with Hugging Face Transformers</a></li>
	  <li><b>Hugginface.com</b>: <a href="https://huggingface.co/course/chapter1/1" target="_blank">Using Huggingface Transformer Models</a></li>		
      </ol>	

    </td>
  </tr>
  

  <tr>
    <td>Week 12 <b>05/06/2023</b></td>
    <td>
      Natural Language Generation (NLG)
      <br>
    </td>
    <td>
    Suggested readings:
      <ol>
	<li><b>Stanford CS224N (2023)</b>: [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture10-nlg.pdf" target="_blank">slides</a>] </li>								
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/24.pdf" target="_blank">Chapter 24 - Dialogue Systems and Chatbots</a> (textbook chapter)</li>
	<li>Liu et al (2016):    <a href="https://arxiv.org/abs/1603.08023.pdf" target="_blank">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a></li>
	<li>Kottur et al (2017): <a href="https://www.ijcai.org/proceedings/2017/0521.pdf" target="_blank">Exploring Personalized Neural Conversational Models</a></li>		
	<li>Gao et al (2019):    <a href="http://arxiv.org/abs/1809.08267" target="_blank">Neural Approaches to Conversational AI</a></li>		
      </ol>
		
    Additional readings:
      <ol>
		<li>Haixun Wang (2018): <a href="https://medium.com/x8-the-ai-community/a-reading-list-and-mini-survey-of-conversational-ai-32fceea97180" target="_blank">An Annotated Reading List of Conversational AI</a> (tutorial)</li>
      </ol>		
    </td>
  </tr>
  

  <tr>
    <td>Week 13 <b>12/06/2023</b></td>
    <td>
      Large Language Models (LLM)
      <br>
    </td>
    <td>
    Suggested readings:
      <ol>
	<li>Brown et al (2020): <a href="https://openai.com/research/language-models-are-few-shot-learners" target="_blank">Language Models are Few-Shot Learners</a></li>
	<li>Wei et al (2021):   <a href="https://arxiv.org/abs/2109.01652" target="_blank">Finetuned Language Models Are Zero-Shot Learners</a></li>		
      </ol>
		
    Additional readings:
      <ol>
	 <li>Raschka (2023): <a href="https://sebastianraschka.com/blog/2023/llm-reading-list.html" target="_blank">Understanding Large Language Models -- A Transformative Reading List</a> (tutorial)</li>
      </ol>		

		
    Python code:
      <ol>	  
	  <li><b>OpenAI.com</b>: <a href="https://github.com/openai/openai-cookbook/blob/main/examples/Zero-shot_classification_with_embeddings.ipynb" target="blank">Zero-shot classification with embeddings</a></li>		
      </ol>
		
    </td>		
  </tr>
  
		
  <tr>
    <td>Week 14 <b>19/06/2023</b></td>
    <td>
      Student Presentations
      <br>
    </td>
    <td>		
		
      Topics:
      <ol>
        <li>Student #1: <a href="" target="_blank">Topic #1</a> </li>
        <li>Student #2: <a href="" target="_blank">Topic #2</a> </li>
        <li>Student #3: <a href="" target="_blank">Topic #3</a> </li>		
      </ol>
    </td>
  </tr>
 
  <tr class="warning">
    <td>-</td>
    <td>
      NB! Practical Tips for NLP Projects
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/readings/final-project-practical-tips.pdf" target="_blank">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html" target="_blank">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
  </tr>

  
  </tbody>
</table>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
