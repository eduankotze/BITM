<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
  <title>CSIA7915</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" type="text/css" href="style.css" />

</head>

<body>

<!-- Header -->
<div id="header" style="text-align:center">
  <h1>Natural Language Processing (CSIA7915) - 2021</h1><br>
  <h4>Adopted from <a href="http://web.stanford.edu/class/cs124/" target="_blank">CS124</a> and
  <a href="http://web.stanford.edu/class/cs224n/" target="_blank">CS224n</a> (Stanford University),
  <a href="http://computational-linguistics-class.org/lectures.html" target="_blank">CIS530 Computational Linguistics</a>, and  
  <a href="https://www.uantwerpen.be/en/study/programmes/all-programmes/digital-text-analysis/" target="_blank">Digital Text Analytics</a> (University of Antwerp)</h4>    
  <b>Updated: 2022/01/17</b>
  <div style="clear:both;"></div>
</div>

<!-- Content -->
<div class="container sec" id="content">

  <h2>Prerequisites</h2>
  <ul>
      <li><b>Proficiency in Python</b>
          <p>Become familiar with Python 3 (using Numpy, pandas, NLTK, scikit-learn and Keras).</p>
          <p><u>The following resources are recommended if you new to Python Programming:</u></p>
          <ul><p>
              <li>Youtube (free) - Learn Python in 45 minutes! <a href="https://collab.hpc.ufs.ac.za/index.php/s/cKFTSdtLwnDXyKo" target="_blank">YouTube [74.7MB]</a></li>
              <li>Kaggle introduction to Python (very good): <a href="https://www.kaggle.com/learn/python" target="_blank">Kaggle - Python</a> - Good introduction if you are not familiar with programming.</li>		
			  <li>Google introduction to Python (excellent): <a href="https://developers.google.com/edu/python" target="_blank">Google’s Python Class</a> - If you know how to program but are not proficient in Python, I would recommend going through <b>Google’s Python Class</b> for people who already know how to code, taught by Nick Parlante.</li>	
          </p></ul>
          
          <p><u>Useful Python Resources:</u></p>
          <ul><p>
              <li>Chris Albon. Python Scripts for Machine Learning and NLP: <a href="https://chrisalbon.com/#machine_learning" target="_blank">https://chrisalbon.com/#machine_learning</a></li>
              <li>Jason Browlee. Machine Learning Mastery with Python: <a href="https://machinelearningmastery.com/" target="_blank">https://machinelearningmastery.com/</a></li>
              <li>MLwhiz. NLP Learning Series: <a href="https://mlwhiz.com/nlpseries/" target="_blank">https://mlwhiz.com/nlpseries/</a></li>
              <li>TowardsDataScience: Machine Learning and NLP: <a href="https://towardsdatascience.com/machine-learning/home" target="_blank">https://towardsdatascience.com/machine-learning/home</a></li>
              <li>NLTK: <a href="https://www.nltk.org/" target="_blank">https://www.nltk.org/</a> - Natural Langauge Toolkit (Bird,2009)</li>
			  <li>Gensim: <a href="https://pypi.org/project/gensim/" target="_blank">https://pypi.org/project/gensim/</a> - Gensim [library for topic modelling, document indexing and similarity retrieval with large corpora]:</li>
			  <li>scikit-learn: <a href="https://scikit-learn.org/stable/" target="_blank">https://scikit-learn.org/stable/</a>
			  The Scikit-Learn website has an impressive breadth of documentation and examples covering some of the models discussed here, and much, much more. 
			  If you want a brief survey of the most important and often-used machine learning algorithms, this website is a good place to start.</li>
          </p></ul>
		  
          <p><u>Useful Keras (Python) Resources:</u></p>
          <ul><p>
		      <li>Keras introduction: <a href="https://chrisalbon.com/" target="_blank">https://chrisalbon.com/</a> (scroll down for the Keras notebooks)</li>
              <li>Keras blog: <a href="https://blog.keras.io" target="_blank">https://blog.keras.io</a> (very useful examples using Keras REST API)</li>	
              <li>Keras Home Page: <a href="https://keras.io/examples/" target="_blank">https://keras.io/examples/</a> (examples by Chollet himself)</li>				  
              <li>Keras Github: <a href="https://github.com/keras-team/keras/tree/master/examples" target="_blank">https://github.com/keras-team/keras/tree/master/examples</a> (these are very useful examples)</li>
          </p></ul>		  
      </li>
      
      <li><b>Calculus, Linear Algebra</b>
          <p>You should be comfortable taking (multivariable) derivatives and understanding matrix/vector notation and operations.</p>
      </li>
      <li><b>Basic Probability and Statistics</b>
          <p>You should know basics of probabilities, gaussian distributions, mean, standard deviation, etc.</p>
      </li>
      <li><b>Foundations of Machine Learning</b>
          <p>You should know the basics of machine learning.
             There are many introductions to ML, in webpage, book, and video form. One approachable introduction is Hal Daum&eacute;’s in-progress <a href="http://ciml.info"><i>A Course in Machine Learning</i></a>. Reading the first 5 chapters of that book would be good background. Knowing the first 7 chapters would be even better!</p>
      </li>
  </ul>

  <h2>Reference Texts</h2>
  <p>
    The following texts are useful, and all of them can be read free online:
  </p>
  <ul>
      <li>Dan Jurafsky and James H. Martin. <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Language Processing (3rd ed. draft)</a> [free downloads]</li>
      <li>Jacob Eisenstein. <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank">Natural Language Processing</a> [free download]</li>
      <li>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. <a href="http://www.deeplearningbook.org/" target="_blank">Deep Learning</a></li>
  </ul>
    <p>
    If you have no background in machine learning, the following book is <b>required reading</b>:
    </p>
    <ul>
      <li>
      Sebastian Raschka and Vahid Mirjalili. <a href="https://sebastianraschka.com/books.html" target="_blank">Python Machine Learning. 3rd edition. 2019. Packt</a>
      </li>
  </ul>
    <p>
    If you have no background in neural networks, you might find one of these books helpful to give you more background:
    </p>
    <ul>
      <li>
      Michael A. Nielsen. <a href="http://neuralnetworksanddeeplearning.com" target="_blank">Neural Networks and Deep Learning</a>
      </li>
      <li>
      Eugene Charniak. <a href="https://mitpress.mit.edu/books/introduction-deep-learning" target="_blank">Introduction to Deep Learning</a>
      </li>
  </ul>  
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Online Courses</h2>
<p>
The following courses I found useful in learning about machine learning and deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>#</th>
    <th>Description</th>
    <th>Online Course</th>
  </tr>
  </thead>
  <tbody>
   
  <tr>
    <td>1.1</td>
    <td>Introduction to General Machine Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>CIML.info:<a href="ttp://ciml.info/" target="_blank">Machine Learning: Reading first 7 chapters of "A Course on Machine Learning"</a> (recommended by Stanford)</li>
        <li>Coursera: <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank">Machine Learning by Prof Andrew Ng (Stanford University)</a>  
		This is a very clearly-taught free online course which covers the basics of machine learning from an algorithmic perspective. 
		It assumes undergraduate-level understanding of mathematics and programming, and steps through detailed considerations of some of the most important machine learning algorithms. 
		Homework assignments, which are algorithmically graded, have you actually implement some of these models yourself.</li>
      </ol>
      Suggested Online Tutorials:
      <ol>
        <li>Intro to Machine Learning <b>(Kaggle)</b>: <a href="https://www.kaggle.com/learn/intro-to-machine-learning" target="_blank">https://www.kaggle.com/learn/intro-to-machine-learning</a> (covers how models work, model validation, underfitting, overfitting, random forest)</li>
        <li>Intermediate Machine Learning <b>(Kaggle)</b>: <a href="https://www.kaggle.com/learn/intermediate-machine-learning" target="_blank">https://www.kaggle.com/learn/intermediate-machine-learning</a> (covers missing values, categorical variables, pipelines, cross-validation, XGBoost, data leakage)</li>
      </ol>
    </td>	
  </tr>

  <tr>
    <td>1.2</td>
    <td>Introduction to General Deep Learning
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Introduction to Deep Learning by National Research University Higher School of Economics, Coursera: <a href="https://www.coursera.org/learn/intro-to-deep-learning" target="_blank">https://www.coursera.org/learn/intro-to-deep-learning</a></li>
      </ol>
    </td>
  </tr>
  
  <tr>
    <td>2.1</td>
    <td>Introduction to NLP (with Python)
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>NLP with Python for Machine Learning Essential Training, LinkedIn Training: <a href="https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/welcome?u=37069596" target="_blank">https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training</a> [UFS login required]</li>
      </ol>
    </td>
  </tr>
  
  <tr>
    <td>2.2</td>
    <td>Advanced NLP (with Python)
    </td>
    <td>
      Suggested Online Courses:
      <ol>
        <li>Natural Language Processing by National Research University Higher School of Economics, Coursera: <a href="https://www.coursera.org/learn/language-processing" target="_blank">https://www.coursera.org/learn/language-processing</a></li>
      </ol>
    </td>
  </tr>
  
  </tbody>
</table>
</div>


<!-- Content -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">
<h2>Content</h2>
<p>
The learning material I consider important theoretical background if you want to become involve in NLP with machine learning and/or deep learning.
</p>
<table class="table">
  <colgroup>
    <col style="width:5%">
    <col style="width:25%">
    <col style="width:70%">
  </colgroup>
  <thead>
  <tr class="active">
    <th>Week</th>
    <th>Description</th>
    <th>Learning Materials</th>
  </tr>
  </thead>
  
 <tr>
    <td>0</td>
    <td>Introduction to the module
	<br>
	Intro 22 Feb [<a href="https://github.com/eduankotze/eduankotze.github.io/blob/main/slides/CSIA7915_L0.pdf" target="_blank">slides</a>]<br>
    </td>
    </td>
    <td>
      Suggested Reading: CSIA7915 Module Guide.
    </td>
  </tr>
	
  <tr>
    <td>1</td>
    <td>Introduction to Data Science and Machine Learning
    </td>
    </td>
    <td>
      Suggested Reading: Jake VanderPlas, Python Data Science Handbook. Essential Tools for Working with Data. O'Reilly Media (2016):
      <ol>
        <li>Introduction to Data Science: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html" target="_blank">Introduction to NumPy</a></li>
        <li>Introduction to Data Science: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html" target="_blank">Data Manipulation with Pandas</a></li>
	<li>Introduction to Data Science: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html" target="_blank">Visualization with Matplotlib</a></li>
      </ol>
    </td>
  </tr>

  <td>2</td>
    <td>Machine Learning: Methodology
    </td>
    <td>
	  
      Suggested Reading: Jake VanderPlas, Python Data Science Handbook. Essential Tools for Working with Data. O'Reilly Media (2016):
      <ol>
		<li>Introduction to Machine Learning: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.html" target="_blank">What is Machine Learning?</a></li>
		<li>Introduction to Machine Learning: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html" target="_blank">Introducing Scikit-learn</a></li>
		<li>Introduction to Machine Learning: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html" target="_blank">Hyperparameters and Model Validation</a></li>		
		<li>Introduction to Machine Learning: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html" target="_blank">Feature Engineering</a></li>		
      </ol>
	  
	  <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	  <ol>
	  	<li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch01/ch01.ipynb" target="_blank">Chapter 1 - Giving Computers the Ability to Learn from Data</a></li>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch04/ch04.ipynb" target="_blank">Chapter 4 - Building Good Training Datasets – Data Preprocessing</a></li>
		<li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch06/ch06.ipynb" target="_blank">Chapter 6 - Learning Best Practices for Model Evaluation and Hyperparameter Tuning</a></li>
	  </ol>
	  
	  Scikit-learn:
      <ol>
	  	<li>Preprocessing: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" target="_blank">Normalization (MinMaxScalar)</a></li>
		<li>Proprocessing: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" target="_blank">Standardization</a></li>
	
	    <li>Cross-validation: <a href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank">Cross-validation: evaluating estimator performance</a></li>
		<li>Cross-validation: <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html" target="_blank">Visualizing cross-validation behavior in scikit-learn</a></li>
        <li>Hyperparameters: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" target="blank">GridSearchCV</a></li>
      </ol>	  
    </td>
  </tr>
  
  <td>3</td>
    <td>Machine Learning: Methods / Algoritms
    </td>
    <td>
  
      Suggested Reading: Jake VanderPlas, Python Data Science Handbook. Essential Tools for Working with Data. O'Reilly Media (2016):
      <ol>
		<li>Methods: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html" target="_blank">Naive Bayes</a></li>
		<li>Methods: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html" target="_blank">Support Vector Machines</a></li>
		<li>Methods: <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html" target="_blank">Decision Trees/Forests</a></li>		
      </ol>
	  
	  <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	  <ol>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb" target="_blank">Chapter 8 - Applying Machine Learning To Sentiment Analysis</a></li>
	  </ol>
	  
    </td>
  </tr>
   
   <tr>
    <td>4</td>
    <td>Introduction to Neural Networks
    </td>
    <td>
      Suggested Readings:
      <ol>
	    <li><a href="http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/" target="_blank">Visual and Interactive Guide to the Basics of Neural Networks</a> (a tutorial by Jay Alammar)</li>
	    <li><a href="http://jalammar.github.io/feedforward-neural-networks-visual-interactive/" target="_blank">Visual and Interactive Look at Basic Neural Network Math</a> (a tutorial by Jay Alammar)</li>
      </ol>

	  <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	  <ol>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch15/ch15_part1.ipynb" target="_blank">Chapter 15: Classifying Images with Deep Convolutional Neural Networks (Part 1/2)</a></li>
	  </ol>
	  
    </td>
  </tr>
  
  
  <thead>
  <tr class="active">
    <th>Week</th>
    <th>Description</th>
    <th>Learning Materials</th>
  </tr>
  </thead> 
  

  <tr>
    <td>5</td>
    <td>NLP and Language Modeling
    <td>
  
	  Suggested Reading:
      <ol>
        <li>Wikipedia (n.d.): <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">Brief history of Natural Language Processing</a></li>
      </ol>
	  Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href=" https://web.stanford.edu/~jurafsky/slp3/2.pdf" target="_blank">Chapter 2 - Regular Expressions, Text Normalization, and Edit Distance</a> (textbook chapter)</li>
      </ol>
	  
      Suggested Reading:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter). 
		<font color="red">Pages 1-16 (plus section 3.7 "Huge Language Models and Stupid Backoff")</font></li>
      </ol>
	  
	  Python examples:
      <ol>
	    <li>Gensim: <a href="https://radimrehurek.com/gensim/parsing/preprocessing.html" target="_blank">Preprocessing raw text</a></li>
	    <li>NLTK: <a href="https://www.kaggle.com/awadhi123/text-preprocessing-using-nltk/" target="_blank">Text preprocessing using NLTK in Python</a></li>
	    <li>Feature Engineering: <a href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" target="blank">Text Feature Extraction (BoW, TFIDF, n-grams)</a></li>
      </ol>	 
	  
    </td>
  </tr>

  <tr>
    <td>6</td>
    <td>Text Classification and Sentiment Analysis
	<br>
		  NB [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf" target="_blank">slides</a>]<br>
		  Sentiment [<a href="https://web.stanford.edu/~jurafsky/slp3/slides/7_Sent.pdf" target="_blank">slides</a>]
    </td>
    <td>
      Suggested Reading - Naive Bayes and Text Classification:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf" target="_blank">Chapter 4 - Naive Bayes Classification and Sentiment Classification</a> (textbook chapter). 
		<font color="red">pages 1-14 plus page 18, sections 4.1 through 4.8 and 4.10</font></li>
	<li>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. (2002): <a href=http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf" target="_blank">Thumbs up? Sentiment Classification using Machine Learning Techniques. </a>EMNLP 2002, pages 79-86</li>	      
      </ol>
	  
      Suggested Reading - Logistic Regression:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf" target="_blank">Chapter 5 - Logistic Regression</a> (textbook chapter). 
		<font color="red">pages 1-14. Pages 18-19 may also be useful! </font></li>		
      </ol>	 
	  
	  <u>Rehash: Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
	  <ol>
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch08/ch08.ipynb" target="_blank">Chapter 8 - Applying Machine Learning To Sentiment Analysis</a></li>
	  </ol>
	  
     </td>
  </tr>
 
  <tr>
    <td>7</td>
    <td>Word Vectors
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture01-wordvecs1.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf" target="_blank">notes (lecture 1)</a>]
    </td>
    <td>
      
		
      Suggested Readings:
      <ol>
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Chapter 6 - Vector Semantics and Embeddings</a> (textbook chapter). 
		<font color="red">pages 1-7, 17-26</font></li>
        <li>Mikolov et al (2013a): <a href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a> (original <b>word2vec</b> paper)</li>
        <li>Mikolov et al (2013b): <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a> (negative sampling paper)</li>
	<li>Le et al (2014): <a href="https://arxiv.org/abs/1405.4053" target="_blank">Distributed Representations of Sentences and Documents</a> (original <b>doc2vec</b> paper)</li>
	<li>Joulin et al (2016): <a href="https://arxiv.org/abs/1607.01759" target="_blank">Bag of Tricks for Efficient Text Classification</a> (use this citation if using <b>FastText</b> for text classification)</li>
	<li>Enriquez et al (2016): <a href="https://www.sciencedirect.com/science/article/pii/S0957417416304833?via%3Dihub" target="_blank">An approach to the use of word embeddings in an opinion classification task</a> (paper to show how to use word2vec)</li>		
      </ol>		

		
      Suggested Tutorials:
      <ol>
        <li><a href="http://jalammar.github.io/illustrated-word2vec/" target="_blank">The Illustrated Word2Vec</a> (tutorial by Jay Alammar)</li>		
        <li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank">Word2Vec Tutorial - The Skip-Gram Model</a> (tutorial by McCormick)</li>
      </ol>	
		
      Python code using word, sentence and document embeddings:
      <ol>	
		<li>Gensim Word2Vec Demo: <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html" target="_blank">Word2Vec Model</a></li>
		<li>Gensim FastText Demo (sentence): <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html" target="_blank">FastText Model</a></li>
		<li>Gensim Doc2Vec Demo (document): <a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html" target="_blank">Doc2Vec Model</a></li>
		<li>Official FastText by Facebook: <a href="https://github.com/facebookresearch/fastText">fastText github</a></li>
		<li>Kaggle: <a href="https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial">Gensim Word2Vec Tutorial</a></li>
      </ol>
    </td>
  </tr>

  <tr>
    <td>8</td>
    <td>Word Vectors 2 and Word Senses
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf" target="_blank">slides</a>]
      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf" target="_blank">notes (lecture 2)</a>]<br>
	  [<a href="http://nlp.stanford.edu/projects/glove/" target="_blank">glove pretrained</a>]
      <br><br>
      Gensim word vectors example (GloVe):
      [<a href="http://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html" target="_blank">preview</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li>Pennington et al (2014): <a href="http://aclweb.org/anthology/D/D14/D14-1162.pdf" target="_blank">GloVe: Global Vectors for Word Representation</a> (original GloVe paper)</li>
        <li>Levy et al (2015): <a href="http://www.aclweb.org/anthology/Q15-1016" target="_blank">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a></li>
        <li>Schnabel et al (2015): <a href="http://www.aclweb.org/anthology/D15-1036" target="_blank">Evaluation methods for unsupervised word embeddings</a></li>
        <li>Bonanowki et al (2017): <a href="https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00051?mobileUi=0&" target="_blank">Enriching Word Vectors with Subword Infomration</a> (original FastText paper)</li>
	<li>Mikolov et al (2018): <a href="https://arxiv.org/abs/1712.09405" target="_blank">Advances in Pre-Training Distributed Word Representations</a></li>		
      </ol>
	  
      Additional Readings:
      <ol>
      	<li><a href="https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf" target="_blank">On the Dimensionality of Word Embedding.</a></li>
      </ol>
	  
      Python code of word embeddings:
      <ol>	  
	    <li><b>Keras.IO: </b><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank">Using pre-trained word embeddings in a Keras model</a> (2016) (by Francois Chollet)</li>	  
	    <li><b>Keras.IO: </b><a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" target="_blank">Using pre-trained word embeddings</a> (2021) (by Francois Chollet)</li>	  
      </ol>
	  
    </td>
  </tr>

  
  <tr>
    <td>9</td>

    <td>Neural Network Languge Models, Recurrent Neural Networks (RNN), Language Models (LM).
	<br>
		[<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
    <td>

	Revision:
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Chapter 3 - N-gram Language Models</a> (textbook chapter).</li>
      </ol>
		
	Suggested Readings (part 1):
      <ol>
        <li>Bengio (2003): <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank"> A Neural Probabilistic Language Model</a> (original paper)</li>
	<li>Sebastian Ruder (2018): <a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/" target="_blank">A Review of the Neural History of Natural Language Processing</a></li>	      
	<li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf" target="_blank">Chapter 7 - Neural Networks and Neural Language Models</a> (textbook chapter)</li>
      </ol>
	  
      Suggested Readings (part 2):
      <ol>
	    <li>Goodfellow et al (2016): <a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank">Chapter 10 - Sequence Modeling: Recurrent and Recursive Neural Nets</a> (<i>Deep Learning</i> book chapter). 
		<font color="red">Sections 10.1 and 10.2</font></li>
        <li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank">Recurrent Neural Networks Tutorial</a> (practical guide by Denny Britz)</li>
      </ol>
 
    </td>
  </tr>
 
  <tr>
    <td>10</td>

    <td>Fancy RNNs, Seq2Seq.
	<br>
		[<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
    <td>

      Suggested Readings (part 1):
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - Deep Learning Architectures for Sequence Processing</a> (textbook chapter). 
		<font color="red">Sections 9.1 - 9.3</font></li>

      </ol>
		
      Suggested Readings (part 2):
      <ol>
        <li>Goodfellow et al (2016): <a href="http://www.deeplearningbook.org/contents/rnn.html" target="_blank">Chapter 10 - Sequence Modeling: Recurrent and Recursive Neural Nets</a> (<i>Deep Learning</i> book chapter). 
		<font color="red">Sections 10.3, 10.5, 10.7-10.12</font></li>
	<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Understanding LSTM Networks</a> (blog post overview by Colah)</li>
      </ol>
	  
	  <u>Sebastian Raschka and Vahid Mirjalili. Python Machine Learning. 3rd edition. Packt (2019):</u>
      <ol>	  
	    <li><a href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch16/ch16_part1.ipynb" target="_blank">Chapter 16: Modeling Sequential Data Using Recurrent Neural Networks (Part 1/2)</a> - RNN layers (SimpleRNN, LSTM, GRU) <b>[see project one]</b>:</li>	  
      </ol> 

	  Python code of deep learning language models:
      <ol>	  
	    <li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" target="blank">Bidirectional LSTM on IMDB</a> (by Francois Chollet)</li>
	    <li><b>Good introduction: </b><a href="https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/" target="_blank">TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras</a> (by Jason Brownlee)</li>	  
      </ol> 
	  
    </td>
  </tr>
		
  <tr>
    <td>11</td>
    <td>CNNs for NLP <br>
	
	      [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
	      [<a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes08-CNN.pdf" target="_blank">notes</a>]		  
	  
    </td>
    <td>
      Suggested Readings:
      <ol>
	<li>Goodfellow et al (2016): <a href="http://www.deeplearningbook.org/contents/convnets.html" target="_blank">Chapter 9 - Convolutional Networks (CNN)</a> (<i>Deep Learning</i> book chapter)</li>
        <li>Kim (2014): <a href="https://arxiv.org/abs/1408.5882.pdf" target="_blank">Convolutional Neural Networks for Sentence Classification</a></li>
        <li>Hinton et al (2012): <a href="https://arxiv.org/abs/1207.0580" target="_blank">Improving neural networks by preventing co-adaptation of feature detectors</a></li>
        <li>Kalchbrenner et al (2014): <a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank">A Convolutional Neural Network for Modelling Sentences</a></li>
      </ol>
	  
 	  Python code of deep learning language models:
      <ol>	  
		<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" target="blank">CNN using pre-trained (GLoVe) word embeddings</a></li>
		<li><b>Very good summary</b>: <a href="https://realpython.com/python-keras-text-classification/" target="blank">Practical Text Classification With Python and Keras</a></li>		
      </ol>	
    </td>
  </tr>

  
 <tr>
    <td>12</td>
    <td>Transformers<br>
    [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
  </td>
    <td>
	
      Suggested Readings:
      <ol>
        <li>Jurafsky and Martin (2020): <a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf" target="_blank">Chapter 9 - Deep  Learning  Architecturesfor Sequence Processing</a> (textbook chapter). 
		<font color="red">Sections 9.4 - 9.5</font></li>
      </ol>	

      Suggested Tutorials:
      <ol>
    	<li><a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a> (tutorial by Jay Alammar)</li>
        <li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a> (tutorial by Jay Alammar)</li>	     
	<li><a href="https://machinelearningmastery.com/the-transformer-model/" target="_blank">Discovering the network architecture of the Transformer Model</a> (tutorial by Stefania Cristina)</li>
        <li><a href="http://www.davidsbatista.net/blog/2020/01/25/Attention-seq2seq/" target="_blank">The Attention Mechanism in Natural Language Processing - seq2seq</a> (tutorial by David. S Batista)</li> 
      </ol>	
		
      Suggested Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/1706.03762.pdf" target="_blank">Attention Is All You Need</a> (seminal paper)</li>

        <li><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank">Transformer (Google AI blog post)</a></li>
        <li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank">Layer Normalization</a></li>
        <li><a href="https://arxiv.org/pdf/1802.05751.pdf" target="_blank">Image Transformer</a></li>
      </ol>
	  
	  Python code:
      <ol>	  
		<li><b>Keras.IO</b>: <a href="https://keras.io/examples/nlp/text_classification_with_transformer/" target="blank">Text classification with Transformer</a></li>
      </ol>	
    </td>
  </tr>

 
  
  <tr>
    <td>13</td>
    <td>More about Transformers and Pretraining
    <br>
      [<a href="http://web.stanford.edu/class/cs224n/slides/" target="_blank">slides</a>]
    <td>
	
	  Background:
	  <ol>
        <li><b>ELMo</b> (Peters et al., 2018): In traditional word embeddings such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Mikolov et al., 2018), 
		each word in the vocabulary is mapped to a fixed vector, regardless of its context. 
		The core idea of ELMo is to address this weakness by using the context in which a word appears to construct the embedding.
		In practice, ELMo implements this idea by training a two-layer bidirectional LSTM for language modeling on a large-scale corpus. 
		This pretrained bi-LSTM can then be used as the embedding layer for any model which takes text as input. 
		The remaining layers of the new model are trained for the new task.
		When it was released in November 2017, ELMo improved the state of the art on six different NLP benchmarks.  
		This established the utility of pretrained contextual embeddings (PCE), but many architectural advances have happened since November 2017. 
		As such, ELMo is unlikely to compete with more recent PCE methods such as BERT.
		</li>
		<li><b>BERT</b> (Devlin et al., 2018): Transformers (Vaswani et al., 2017) have in some cases supplanted RNNs as the dominant model family in deep learning for NLP. 
		Therefore one might expect that ELMo’s pretrained RNNs would be outperformed by pretrained Transformers.  
		Indeed, BERT showed exactly that by training deep Transformers on a carefully designed bidirectional language modeling task, 
		BERT achieves state-of-the-art results on a wide variety of NLP benchmark. <b>BERT</b> stands for Bidirectional Encoder Representations from Transformers.
		</li>
		<li><b>ALBERT</b> (Lan et al., 2020): More recently, ALBERT established new state-of-the-art results on various benchmarks while having fewer parameters compared to BERT-large. 
		Their key idea is to allocatethe model’s capacity more efficiently and to share parameters across the layers. 
		Like BERT, the authors released an open-source TensorFlow implementation and pretrained weights.
		<b>ALBERT</b> stands for A Lite BERT.
		
      </ol>
	  
	  Suggested readings (BERT):
      <ol>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf" target="_blank">Chapter 10 - Encoder-Decoder Models</a> (textbook chapter)</li>
        <li>Jurafsky and Martin (3rd): <a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf" target="_blank">Chapter 11 - Transfer Learning with Pre-trained Language Models and Contextual Embeddings</a> (textbook chapter)</li>
			
		
        <li>Devlin et al (2018) <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (original BERT paper)
        </li>
	  	<li><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" target="_blank">A Visual Guide to Using BERT for the First Time</a> (tutorial by Jay Alammar)</li>		
      </ol>
	  
	  Suggested readings (Contextual Word Representations):
      <ol>
	    <li><a href="http://www.davidsbatista.net//blog/2018/12/06/Word_Embeddings/" target="_blank">Language Models and Contextualised Word Embeddings</a> (tutorial by David S. Batista)</li>
	    <li><a href="http://jalammar.github.io/illustrated-bert/" target="_blank">The Illustrated BERT, ELMo, and co.</a> (tutorial by Jay Alammar)</li>
	    <li><a href="http://jalammar.github.io/illustrated-gpt2/" target="_blank">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> (tutorial by Jay Alammar)</li>
	    <li>Peters et al (2018): <a href="http://aclweb.org/anthology/N18-1202" target="blank">Deep Contextualized Word Representations</a> (original ELMo paper)</li>
	    <li>Smith (2019): <a href="https://arxiv.org/abs/1902.06006.pdf" target="blank">Contextual Word Representations: A Contextual Introduction</a></li>		
      </ol>
	  
	  Python code:
      <ol>	  
		<li><b>BERT Sentiment Analysis Tutorial</b>: <a href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert/" target="blank">Classify text with BERT</a></li>
      </ol>	

    </td>
  </tr>
  

  <tr>
    <td>13</td>
    <td>
      Conversational Agents (aka "Chatbots")
      <br>
    </td>
    <td>
    Suggested readings:
      <ol>
		<li>Jurafsky and Martin (2020): <a href="https://web.stanford.edu/~jurafsky/slp3/24.pdf" target="_blank">Chapter 24 - Dialogue Systems and Chatbots</a> (textbook chapter)</li>
		<li>Liu et al (2016):    <a href="https://arxiv.org/abs/1603.08023.pdf" target="_blank">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</a></li>
	        <li>Kottur et al (2017): <a href=" https://www.ijcai.org/proceedings/2017/0521.pdf" target="_blank">Exploring Personalized Neural Conversational Models</a></li>		
	        <li>Gao et al (2019):    <a href="http://arxiv.org/abs/1809.08267" target="_blank">Neural Approaches to Conversational AI</a></li>		
      </ol>
    </td>
  </tr>
  
  
 
  <tr class="warning">
    <td>14</td>
    <td>
      NB! Practical Tips for NLP Projects
      <br>
      [<a href="http://web.stanford.edu/class/cs224n/readings/final-project-practical-tips.pdf" target="_blank">notes</a>]
    </td>
    <td>
      Suggested Readings:
      <ol>
        <li><a href="https://www.deeplearningbook.org/contents/guidelines.html" target="_blank">Practical Methodology</a> (<i>Deep Learning</i> book chapter)</li>
      </ol>
    </td>
  </tr>

  
  </tbody>
</table>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
